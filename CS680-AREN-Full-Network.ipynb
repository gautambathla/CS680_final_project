{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import gc\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import tensorflow as tf\n",
    "# import tensorflow.compat.v1 as tf\n",
    "from tensorflow.keras.layers import BatchNormalization, Dense, MaxPool2D, Conv2D, Dropout, GlobalAveragePooling2D, Flatten, Input, Concatenate, GlobalMaxPool2D, Lambda\n",
    "from tensorflow.keras import Sequential, Model\n",
    "from tensorflow.keras.applications import ResNet101\n",
    "from itertools import combinations\n",
    "import keras\n",
    "import tensorflow.keras.backend as K\n",
    "from scipy import spatial\n",
    "import os\n",
    "from PIL import Image\n",
    "import h5py\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import gc\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy import sparse\n",
    "for dirname, _, filenames in os.walk('/kaggle/input/inputdata/'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"../input/caltech-birds-2011-dataset/CUB_200_2011/classes.txt\")\n",
    "class_label_dict = {}\n",
    "for line in f:\n",
    "    items = line.split()\n",
    "    class_label_dict[items[1]] = int(items[0])-1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "global_train_labels = []\n",
    "train_filenames = []\n",
    "f = open(\"/kaggle/input/classeslist/trainvalclasses.txt\")\n",
    "for line in f:\n",
    "    global_train_labels.append(class_label_dict[line.strip()])\n",
    "    train_filenames.append(line.strip())\n",
    "    \n",
    "global_test_labels = []\n",
    "test_filenames = []\n",
    "f = open(\"/kaggle/input/classeslist/testclasses.txt\")\n",
    "for line in f:\n",
    "    global_test_labels.append(class_label_dict[line.strip()])\n",
    "    test_filenames.append(line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop on labels -> feature -> array of weights. Create text features array\n",
    "\n",
    "    \n",
    "f = open(\"/kaggle/input/class-attributes/class_attribute_labels_continuous.txt\")\n",
    "class_features_list = []\n",
    "for line in f:\n",
    "    values = [float(numeric_string) for numeric_string in line.split()]\n",
    "    class_features_list.append(values)\n",
    "    \n",
    "class_features_list = np.array(class_features_list)\n",
    "# len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(directory, folder_names):\n",
    "    \n",
    "    data =[]\n",
    "    filename_list = []\n",
    "    \n",
    "    for folder_name in folder_names:\n",
    "        for dirname, _, filenames in os.walk(os.path.join(directory, folder_name)):\n",
    "            for filename in filenames:\n",
    "                img = np.asarray(Image.open(os.path.join(dirname, filename)).convert('RGB').resize((256,256)))\n",
    "                data.append(img)\n",
    "                filename_list.append(os.path.join(dirname, filename))\n",
    "    \n",
    "    classes = [filename.split('/')[-2] for filename in filename_list]\n",
    "\n",
    "    labels = [class_label_dict[cls] for cls in classes]\n",
    "    \n",
    "    return np.array(data), np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shapes:  (8821, 256, 256, 3) (8821,)\n",
      "Test data shapes:  (2967, 256, 256, 3) (2967,)\n"
     ]
    }
   ],
   "source": [
    "train_data, train_labels = load_data('/kaggle/input/caltech-birds-2011-dataset/CUB_200_2011/images', train_filenames)\n",
    "test_data, test_labels = load_data('/kaggle/input/caltech-birds-2011-dataset/CUB_200_2011/images', test_filenames)\n",
    "\n",
    "print(\"Train data shapes: \", train_data.shape, train_labels.shape)\n",
    "print(\"Test data shapes: \", test_data.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text_features = []\n",
    "training_phase = np.ones(len(train_labels))\n",
    "for label in train_labels:\n",
    "    train_text_features.append(class_features_list[label])\n",
    "    \n",
    "train_text_features = np.array(train_text_features)\n",
    "\n",
    "test_text_features = []\n",
    "testing_phase = np.zeros(len(test_labels))\n",
    "for label in test_labels:\n",
    "    test_text_features.append(class_features_list[label])\n",
    "    \n",
    "test_text_features = np.array(test_text_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_train_labels = np.array(global_train_labels)\n",
    "train_labels_inds = np.arange(len(global_train_labels))\n",
    "\n",
    "train_labels_inds_dict = dict(zip(global_train_labels, train_labels_inds))\n",
    "\n",
    "\n",
    "global_test_labels = np.array(global_test_labels)\n",
    "test_labels_inds = np.arange(len(global_test_labels))\n",
    "\n",
    "test_labels_inds_dict = dict(zip(global_test_labels, test_labels_inds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_weights_matrix = []\n",
    "\n",
    "for i in range(len(global_train_labels)):\n",
    "    train_weights_matrix.append(class_features_list[global_train_labels[i]])\n",
    "    \n",
    "train_weights_matrix = np.array(train_weights_matrix)\n",
    "    \n",
    "test_weights_matrix = []\n",
    "\n",
    "for i in range(len(global_test_labels)):\n",
    "    test_weights_matrix.append(class_features_list[global_test_labels[i]])\n",
    "    \n",
    "test_weights_matrix = np.array(test_weights_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 312)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_weights_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_train_labels = []\n",
    "\n",
    "for i in range(len(train_labels)):\n",
    "    final_train_labels.append(train_labels_inds_dict[train_labels[i]])\n",
    "    \n",
    "final_train_labels = np.array(final_train_labels)\n",
    "    \n",
    "final_test_labels = []\n",
    "\n",
    "for i in range(len(test_labels)):\n",
    "    final_test_labels.append(test_labels_inds_dict[test_labels[i]])\n",
    "\n",
    "final_test_labels = np.array(final_test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_inds = np.random.permutation(len(train_data))\n",
    "\n",
    "train_data_shuffled = train_data[train_data_inds]\n",
    "final_train_labels_shuffled = final_train_labels[train_data_inds]\n",
    "train_text_features_shuffled = train_text_features[train_data_inds]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_sparse = sparse.csr_matrix(class_features_list)\n",
    "\n",
    "similarities = cosine_similarity(A_sparse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0.0\n",
    "        self.avg = 0.0\n",
    "        self.sum = 0.0\n",
    "        self.count = 0.0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "class ClassAverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self, n_labels):\n",
    "        self.reset(n_labels)\n",
    "\n",
    "    def reset(self,n_labels):\n",
    "        self.n_labels = n_labels\n",
    "        self.acc = np.zeros(n_labels)\n",
    "        self.cnt = np.array([1e-8]*n_labels)\n",
    "        self.pred_prob = []\n",
    "\n",
    "    def update(self, val, cnt, pred_prob):\n",
    "        self.acc += val\n",
    "        self.cnt += cnt\n",
    "        self.avg = 100*self.acc.dot(1.0/self.cnt).item()/self.n_labels\n",
    "        self.pred_prob += pred_prob\n",
    "        #print ('pred',len(self.pred_prob))\n",
    "\n",
    "def accuracy(output_vec, target, n_labels):\n",
    "    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n",
    "    output = output_vec\n",
    "\n",
    "    batch_size = target.shape[0]\n",
    "    pred = output.argsort()[:,-1]\n",
    "    class_accuracy = np.zeros(n_labels)\n",
    "    class_cnt = np.zeros(n_labels)\n",
    "    prec = 0.0\n",
    "    pred_prob = []\n",
    "    for i in range(target.shape[0]):\n",
    "        t = target[i]\n",
    "        pred_prob.append(output[i][t])\n",
    "\n",
    "        if pred[i] == tf.cast(t, tf.int32):\n",
    "            prec += 1\n",
    "            class_accuracy[t] += 1\n",
    "        class_cnt[t] += 1\n",
    "    return prec*100.0/batch_size, class_accuracy, class_cnt, pred_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_weights_matrix_numpy = torch.nn.functional.normalize(torch.tensor(train_weights_matrix)).numpy()\n",
    "test_weights_matrix_numpy = torch.nn.functional.normalize(torch.tensor(test_weights_matrix)).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AREN with SE Block and projections onto Shared Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLayer(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, train_weights, test_weights):\n",
    "        super(CustomLayer, self).__init__()\n",
    "        self.train_weights = tf.transpose(train_weights)\n",
    "        self.test_weights = tf.transpose(test_weights)\n",
    "        \n",
    "    def train_step(self, are_features, acse_features):\n",
    "        are_output = tf.matmul(are_features, self.train_weights)\n",
    "        acse_output = tf.matmul(acse_features, self.train_weights)\n",
    "        are_output = tf.keras.activations.relu(are_output)\n",
    "        acse_output = tf.keras.activations.relu(acse_output)\n",
    "        return are_output, acse_output\n",
    "    \n",
    "    def test_step(self, are_features, acse_features):\n",
    "        are_output = tf.matmul(are_features, self.test_weights)\n",
    "        acse_output = tf.matmul(acse_features, self.test_weights)\n",
    "        are_output = tf.keras.activations.relu(are_output)\n",
    "        acse_output = tf.keras.activations.relu(acse_output)\n",
    "        return are_output, acse_output\n",
    "\n",
    "    def call(self, are_features, acse_features, is_training):\n",
    "        return tf.cond(tf.equal(is_training, 1),\n",
    "                       lambda: self.train_step(are_features, acse_features),\n",
    "                       lambda: self.test_step(are_features, acse_features))\n",
    "    \n",
    "class CustomPreprocessing(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(CustomPreprocessing, self).__init__()\n",
    "        self.random_crop = tf.keras.layers.experimental.preprocessing.RandomCrop(224, 224)\n",
    "        self.random_flip = tf.keras.layers.experimental.preprocessing.RandomFlip('horizontal')\n",
    "        self.center_crop = tf.keras.layers.experimental.preprocessing.CenterCrop(224, 224)\n",
    "        self.mean_tensor = tf.constant([0.406, 0.456, 0.485], name=\"mean_tensor\")\n",
    "        self.std_tensor = tf.constant([0.225, 0.224, 0.229], name=\"std_tensor\")\n",
    "        self.preproc_layer = Lambda(lambda x: (tf.cast(x, tf.float32) - self.mean_tensor) / self.std_tensor)\n",
    "        \n",
    "    def train_preprocess(self, image_features):\n",
    "        image_features = self.random_crop(image_features)\n",
    "        image_features = self.random_flip(image_features)\n",
    "        image_features = self.preproc_layer(image_features)\n",
    "        return image_features\n",
    "    \n",
    "    def test_preprocess(self, image_features):\n",
    "        image_features = self.center_crop(image_features)\n",
    "        image_features = self.preproc_layer(image_features)\n",
    "        return image_features\n",
    "\n",
    "    def call(self, image_features, is_training):\n",
    "        return tf.cond(tf.equal(is_training, 1),\n",
    "                       lambda: self.train_preprocess(image_features),\n",
    "                       lambda: self.test_preprocess(image_features))\n",
    "\n",
    "class MyModel(tf.keras.Model):\n",
    "    def __init__(self, k, alpha, cmps):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.resnet_base = ResNet101(include_top=False, input_shape=(224,224,3), weights=\"imagenet\")\n",
    "        self.resnet = Model(inputs=self.resnet_base.inputs, outputs=self.resnet_base.get_layer(\"conv5_block3_3_bn\").output)\n",
    "        self.flatten = Flatten()\n",
    "        self.custom_preprocess = CustomPreprocessing()\n",
    "        self.conv_channels = 2048\n",
    "        self.threshold = alpha\n",
    "        self.parts = k\n",
    "        self.map_size = 7\n",
    "        self.pool2d = MaxPool2D(pool_size=self.map_size, strides=self.map_size)\n",
    "        self.conv2d = Conv2D(self.parts, 1, kernel_initializer=tf.keras.initializers.orthogonal)\n",
    "        self.resnet_features = None\n",
    "        self.resnet_feature_shapes = None\n",
    "        self.conv_bilinear = Conv2D(cmps, 1, kernel_initializer=tf.keras.initializers.orthogonal)#, kernel_initializer=tf.keras.initializers.ConvolutionDeltaOrthogonal)\n",
    "        self.coef = self.map_size * self.map_size\n",
    "        self.p_linear = Dense(1024, use_bias=False, kernel_initializer=tf.keras.initializers.orthogonal)\n",
    "        self.b_linear = Dense(1024, use_bias=False, kernel_initializer=tf.keras.initializers.orthogonal)\n",
    "        self.attr_linear = Dense(1024, use_bias=False, kernel_initializer=tf.keras.initializers.orthogonal)\n",
    "        self.dropout_p = Dropout(0.4)\n",
    "        self.dropout_b = Dropout(0.4)\n",
    "        self.dropout_attr = Dropout(0.4)\n",
    "        self.batchnorm = BatchNormalization()\n",
    "        self.batchnorm_bilinear = BatchNormalization()\n",
    "        self.shared_train_init = tf.constant_initializer(tf.keras.backend.l2_normalize(train_weights_matrix, axis=1).numpy())\n",
    "        self.dense_se_1 = Dense(512, use_bias=False, kernel_initializer=tf.keras.initializers.orthogonal)\n",
    "        self.dense_se_2 = Dense(2048, use_bias=False, kernel_initializer=tf.keras.initializers.orthogonal)\n",
    "        self.gap = GlobalAveragePooling2D()\n",
    "        \n",
    "        self.image_tower_are = Sequential(\n",
    "            [\n",
    "                Dense(1024, use_bias=False, kernel_initializer=tf.keras.initializers.orthogonal),\n",
    "                Dropout(0.25),\n",
    "                Dense(1024, use_bias=False, kernel_initializer=tf.keras.initializers.orthogonal),\n",
    "                Dropout(0.25),\n",
    "                Dense(1024, use_bias=False, kernel_initializer=tf.keras.initializers.orthogonal),\n",
    "                Dropout(0.25),\n",
    "                Dense(1024, use_bias=False, kernel_initializer=tf.keras.initializers.orthogonal),#, kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n",
    "                Dropout(0.25),\n",
    "                Dense(1024, use_bias=False, kernel_initializer=tf.keras.initializers.orthogonal),\n",
    "            ]\n",
    "        )\n",
    "        self.image_tower_acse = Sequential(\n",
    "            [\n",
    "                Dense(1024, use_bias=False, kernel_initializer=tf.keras.initializers.orthogonal),\n",
    "                Dropout(0.25),\n",
    "                Dense(1024, use_bias=False, kernel_initializer=tf.keras.initializers.orthogonal),\n",
    "                Dropout(0.25),\n",
    "                Dense(1024, use_bias=False, kernel_initializer=tf.keras.initializers.orthogonal),\n",
    "                Dropout(0.25),\n",
    "                Dense(1024, use_bias=False, kernel_initializer=tf.keras.initializers.orthogonal),#, kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n",
    "                Dropout(0.25),\n",
    "                Dense(1024, use_bias=False, kernel_initializer=tf.keras.initializers.orthogonal),\n",
    "            ]\n",
    "        )\n",
    "        self.text_tower = Sequential(\n",
    "            [\n",
    "                Dense(1024, use_bias=False, kernel_initializer=tf.keras.initializers.orthogonal),#,, activation='relu'),\n",
    "                Dropout(0.25),\n",
    "                Dense(1024, use_bias=False, kernel_initializer=tf.keras.initializers.orthogonal),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def call(self, input1, input2, input3):\n",
    "        input1 = self.custom_preprocess(input1, input3[0])\n",
    "\n",
    "        feat = self.resnet(input1)\n",
    "        feat = tf.keras.activations.relu(feat)\n",
    "        \n",
    "        # ==== Squeeze and Excitation Block ==== #\n",
    "        # ==== Squeeze Step ==== #\n",
    "        w = self.gap(feat)\n",
    "\n",
    "        # ==== Excitation Step ==== #\n",
    "        w = self.dense_se_1(w)\n",
    "        w = tf.keras.activations.relu(w)\n",
    "        w = self.dense_se_2(w)\n",
    "        w = tf.keras.activations.sigmoid(w)\n",
    "        w = tf.expand_dims(w, 1)\n",
    "        w = tf.expand_dims(w, 1)\n",
    "        w = tf.broadcast_to(w, [tf.shape(feat)[0], tf.shape(feat)[1], tf.shape(feat)[2], tf.shape(feat)[3]])\n",
    "        feat = tf.math.multiply(feat, w)\n",
    "\n",
    "        self.resnet_features = feat\n",
    "        self.resnet_feature_shapes = tf.shape(feat)\n",
    "        \n",
    "        # ==== Self Attention ==== #\n",
    "        att_weight = self.conv2d(feat)\n",
    "        att_weight = tf.keras.activations.sigmoid(att_weight)\n",
    "        \n",
    "        batch = tf.shape(att_weight)[0]\n",
    "        height = tf.convert_to_tensor(att_weight.shape[1])\n",
    "        width = tf.convert_to_tensor(att_weight.shape[2])\n",
    "        channels = tf.convert_to_tensor(att_weight.shape[3])\n",
    "        \n",
    "        global_max_value = tf.math.reduce_max(tf.reshape(att_weight, [batch, -1]), axis=1)\n",
    "        local_max_value = tf.math.reduce_max(tf.reshape(att_weight, [batch, -1, self.parts]), axis=1)\n",
    "\n",
    "        global_max_value = tf.broadcast_to(tf.expand_dims(global_max_value, axis=-1), shape=[batch, channels])\n",
    "        threshold_value = self.threshold * global_max_value\n",
    "\n",
    "        mask = tf.broadcast_to(tf.expand_dims(tf.expand_dims(tf.cast(tf.math.greater_equal(local_max_value, threshold_value), dtype=tf.float32), axis=1), axis=1),\n",
    "                               shape=[batch, height, width, self.parts])\n",
    "\n",
    "        attention_weights = tf.math.multiply(att_weight, mask)\n",
    "        attention_weights = tf.transpose(attention_weights, perm=[3, 0, 1, 2])\n",
    "        attention_weights = tf.expand_dims(attention_weights, axis=2)\n",
    "        attention_weights_shape = tf.shape(attention_weights)\n",
    "        attention_weights = tf.broadcast_to(attention_weights, shape=[attention_weights_shape[0], attention_weights_shape[1], self.conv_channels, attention_weights_shape[3], attention_weights_shape[4]])\n",
    "        attention_weights = tf.transpose(attention_weights, perm=[0, 1, 3, 4, 2])\n",
    "        features = tf.broadcast_to(tf.expand_dims(self.resnet_features, axis=0), [self.parts, self.resnet_feature_shapes[0], self.resnet_feature_shapes[1], self.resnet_feature_shapes[2], self.resnet_feature_shapes[3]])\n",
    "        Y = tf.math.multiply(attention_weights, features)\n",
    "        \n",
    "        # ==== ARE Network ==== #\n",
    "        are_output = tf.map_fn(lambda vec: self.pool2d(vec), Y)\n",
    "        are_output = tf.squeeze(are_output, [2,3])\n",
    "        are_output = tf.transpose(are_output, [1,0,2])\n",
    "        weights_batch = tf.shape(are_output)[0]\n",
    "        weights_channel = are_output.shape[1]\n",
    "        weights_h = are_output.shape[2]\n",
    "        are_output = tf.reshape(are_output, (weights_batch, weights_channel*weights_h))\n",
    "\n",
    "\n",
    "        # ==== ACSE Network ==== #\n",
    "        \n",
    "        Y_shape = tf.shape(Y)\n",
    "        Y_temp = tf.reshape(Y, shape=[Y.shape[0], Y_shape[1], Y.shape[2]*Y.shape[3], Y.shape[4]])\n",
    "        self.bilinear = self.conv_bilinear(self.resnet_features)\n",
    "        batch = tf.shape(self.bilinear)[0]\n",
    "        h = self.bilinear.shape[1]\n",
    "        w = self.bilinear.shape[2]\n",
    "        channels = self.bilinear.shape[3]\n",
    "        X = tf.reshape(self.bilinear, shape=[batch, h*w, channels])\n",
    "        X = tf.transpose(X, [0,2,1])\n",
    "        acse_output = tf.map_fn(lambda vec: (tf.einsum('ikj,ijl -> ikl', X, vec) / self.coef), Y_temp)\n",
    "        acse_output_shape = tf.shape(acse_output)\n",
    "        acse_output = tf.reshape(acse_output, shape=[acse_output.shape[0], acse_output_shape[1],\n",
    "                                                     acse_output.shape[2]*acse_output.shape[3]])\n",
    "        acse_output = tf.math.reduce_max(acse_output, axis=0)\n",
    "\n",
    "        are_feat = self.dropout_p(self.p_linear(are_output))\n",
    "        acse_feat = self.dropout_b(self.b_linear(acse_output))\n",
    "        \n",
    "        \n",
    "        # ==== Image Tower ==== #\n",
    "        are_feat_shared_space = self.image_tower_are(are_output)\n",
    "        acse_feat_shared_space = self.image_tower_acse(acse_output)\n",
    "        \n",
    "        # ==== Text Tower ==== #\n",
    "        attribute_train_shared_space = self.text_tower(train_weights_matrix_numpy)\n",
    "        attribute_test_shared_space = self.text_tower(test_weights_matrix_numpy)\n",
    "        \n",
    "        \n",
    "        are_feat, acse_feat = CustomLayer(attribute_train_shared_space,\n",
    "                                          attribute_test_shared_space)(are_feat_shared_space, acse_feat_shared_space,\n",
    "                                                                       input3[0])\n",
    "\n",
    "        return are_feat, acse_feat\n",
    "\n",
    "    def model(self):\n",
    "        inp = Input(shape=(256,256,3))\n",
    "        inp1 = Input(shape=(150))\n",
    "        inp2 = Input(shape=(312))\n",
    "        inp3 = Input(shape=())\n",
    "        return Model(inputs=(inp, inp1, inp2, inp3), outputs=self.call(inp, inp2, inp3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "c_loss=tf.keras.losses.CategoricalCrossentropy()\n",
    "def classification_loss(combined_output, labels):\n",
    "    combined_output = tf.keras.activations.softmax(combined_output)\n",
    "    return c_loss(labels, combined_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_obj = MyModel(12, 1, 20)\n",
    "model = model_obj.model()\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "cls_loss = (classification_loss(model.outputs[0], model.inputs[1]) +\n",
    "                classification_loss(model.outputs[1], model.inputs[1]))/2\n",
    "\n",
    "model.add_loss(cls_loss)\n",
    "\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate, momentum=0.9, decay=0.0005)\n",
    "model.compile(optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "\n",
    "for layer in model.layers:\n",
    "    if \"custom_layer\" in layer.name:\n",
    "        layer.trainable = False\n",
    "        print(layer.trainable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00001: LearningRateScheduler reducing learning rate to 0.0009950091958869586.\n",
      "Epoch 1/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 4.2961 - custom_layer_accuracy: 0.1127 - custom_layer_1_accuracy: 0.0535Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  34.81631277384564  Class Avg =  34.75091324874889\n",
      "276/276 [==============================] - 104s 378ms/step - loss: 4.2961 - custom_layer_accuracy: 0.1127 - custom_layer_1_accuracy: 0.0535\n",
      "\n",
      "Epoch 00002: LearningRateScheduler reducing learning rate to 0.0010201103220664818.\n",
      "Epoch 2/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 1.9892 - custom_layer_accuracy: 0.4554 - custom_layer_1_accuracy: 0.4242Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  53.387259858442874  Class Avg =  53.26099211798325\n",
      "276/276 [==============================] - 102s 369ms/step - loss: 1.9892 - custom_layer_accuracy: 0.4554 - custom_layer_1_accuracy: 0.4242\n",
      "\n",
      "Epoch 00003: LearningRateScheduler reducing learning rate to 0.001002596883429802.\n",
      "Epoch 3/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 1.2268 - custom_layer_accuracy: 0.6380 - custom_layer_1_accuracy: 0.6253Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  55.47691270643748  Class Avg =  55.50079034195077\n",
      "276/276 [==============================] - 102s 368ms/step - loss: 1.2268 - custom_layer_accuracy: 0.6380 - custom_layer_1_accuracy: 0.6253\n",
      "\n",
      "Epoch 00004: LearningRateScheduler reducing learning rate to 0.0010052522886301307.\n",
      "Epoch 4/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.8898 - custom_layer_accuracy: 0.7225 - custom_layer_1_accuracy: 0.7219Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  55.64543309740478  Class Avg =  55.62311038092294\n",
      "276/276 [==============================] - 102s 368ms/step - loss: 0.8898 - custom_layer_accuracy: 0.7225 - custom_layer_1_accuracy: 0.7219\n",
      "\n",
      "Epoch 00005: LearningRateScheduler reducing learning rate to 0.0010233817293726693.\n",
      "Epoch 5/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.6828 - custom_layer_accuracy: 0.7853 - custom_layer_1_accuracy: 0.7798Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  56.959892146949784  Class Avg =  56.83085522502489\n",
      "276/276 [==============================] - 101s 368ms/step - loss: 0.6828 - custom_layer_accuracy: 0.7853 - custom_layer_1_accuracy: 0.7798\n",
      "\n",
      "Epoch 00006: LearningRateScheduler reducing learning rate to 0.000989563309163846.\n",
      "Epoch 6/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.5436 - custom_layer_accuracy: 0.8229 - custom_layer_1_accuracy: 0.8187Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  60.80215706100438  Class Avg =  60.679762884391884\n",
      "276/276 [==============================] - 101s 368ms/step - loss: 0.5436 - custom_layer_accuracy: 0.8229 - custom_layer_1_accuracy: 0.8187\n",
      "\n",
      "Epoch 00007: LearningRateScheduler reducing learning rate to 0.0009796102363850497.\n",
      "Epoch 7/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.4337 - custom_layer_accuracy: 0.8566 - custom_layer_1_accuracy: 0.8576Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  55.5443208628244  Class Avg =  55.526205542893564\n",
      "276/276 [==============================] - 102s 368ms/step - loss: 0.4337 - custom_layer_accuracy: 0.8566 - custom_layer_1_accuracy: 0.8576\n",
      "\n",
      "Epoch 00008: LearningRateScheduler reducing learning rate to 0.0009958234798216588.\n",
      "Epoch 8/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.3711 - custom_layer_accuracy: 0.8726 - custom_layer_1_accuracy: 0.8758Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  59.11695315133131  Class Avg =  58.99726178062011\n",
      "276/276 [==============================] - 102s 368ms/step - loss: 0.3711 - custom_layer_accuracy: 0.8726 - custom_layer_1_accuracy: 0.8758\n",
      "\n",
      "Epoch 00009: LearningRateScheduler reducing learning rate to 0.0009946632705018523.\n",
      "Epoch 9/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.3148 - custom_layer_accuracy: 0.8997 - custom_layer_1_accuracy: 0.8958Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  59.99325918436131  Class Avg =  59.980934096817606\n",
      "276/276 [==============================] - 102s 369ms/step - loss: 0.3148 - custom_layer_accuracy: 0.8997 - custom_layer_1_accuracy: 0.8958\n",
      "\n",
      "Epoch 00010: LearningRateScheduler reducing learning rate to 0.0009785208878530511.\n",
      "Epoch 10/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.2627 - custom_layer_accuracy: 0.9126 - custom_layer_1_accuracy: 0.9126Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  59.858442871587464  Class Avg =  59.85029893290043\n",
      "276/276 [==============================] - 102s 369ms/step - loss: 0.2627 - custom_layer_accuracy: 0.9126 - custom_layer_1_accuracy: 0.9126\n",
      "\n",
      "Epoch 00011: LearningRateScheduler reducing learning rate to 0.0010415268478863849.\n",
      "Epoch 11/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.2316 - custom_layer_accuracy: 0.9262 - custom_layer_1_accuracy: 0.9251Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  60.76845298281092  Class Avg =  60.69131252118495\n",
      "276/276 [==============================] - 102s 369ms/step - loss: 0.2316 - custom_layer_accuracy: 0.9262 - custom_layer_1_accuracy: 0.9251\n",
      "\n",
      "Epoch 00012: LearningRateScheduler reducing learning rate to 0.0010009385159699428.\n",
      "Epoch 12/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.2041 - custom_layer_accuracy: 0.9321 - custom_layer_1_accuracy: 0.9344Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  60.97067745197169  Class Avg =  60.972482840754566\n",
      "276/276 [==============================] - 102s 369ms/step - loss: 0.2041 - custom_layer_accuracy: 0.9321 - custom_layer_1_accuracy: 0.9344\n",
      "\n",
      "Epoch 00013: LearningRateScheduler reducing learning rate to 0.0009781128216105903.\n",
      "Epoch 13/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.1651 - custom_layer_accuracy: 0.9454 - custom_layer_1_accuracy: 0.9454Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  59.858442871587464  Class Avg =  59.81996512749829\n",
      "276/276 [==============================] - 102s 369ms/step - loss: 0.1651 - custom_layer_accuracy: 0.9454 - custom_layer_1_accuracy: 0.9454\n",
      "\n",
      "Epoch 00014: LearningRateScheduler reducing learning rate to 0.0009974642699405278.\n",
      "Epoch 14/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.1598 - custom_layer_accuracy: 0.9477 - custom_layer_1_accuracy: 0.9502Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  60.330299966295925  Class Avg =  60.25247809909895\n",
      "276/276 [==============================] - 102s 369ms/step - loss: 0.1598 - custom_layer_accuracy: 0.9477 - custom_layer_1_accuracy: 0.9502\n",
      "\n",
      "Epoch 00015: LearningRateScheduler reducing learning rate to 0.0010139503053179268.\n",
      "Epoch 15/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.1433 - custom_layer_accuracy: 0.9570 - custom_layer_1_accuracy: 0.9553Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  60.06066734074823  Class Avg =  60.01018776593317\n",
      "276/276 [==============================] - 102s 368ms/step - loss: 0.1433 - custom_layer_accuracy: 0.9570 - custom_layer_1_accuracy: 0.9553\n",
      "\n",
      "Epoch 00016: LearningRateScheduler reducing learning rate to 0.0009934217276640449.\n",
      "Epoch 16/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.1251 - custom_layer_accuracy: 0.9613 - custom_layer_1_accuracy: 0.9619Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  61.64475901584092  Class Avg =  61.60666165193036\n",
      "276/276 [==============================] - 102s 369ms/step - loss: 0.1251 - custom_layer_accuracy: 0.9613 - custom_layer_1_accuracy: 0.9619\n",
      "\n",
      "Epoch 00017: LearningRateScheduler reducing learning rate to 0.0010119386311607378.\n",
      "Epoch 17/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.1105 - custom_layer_accuracy: 0.9657 - custom_layer_1_accuracy: 0.9649Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  61.375126390293225  Class Avg =  61.245535515593744\n",
      "276/276 [==============================] - 102s 369ms/step - loss: 0.1105 - custom_layer_accuracy: 0.9657 - custom_layer_1_accuracy: 0.9649\n",
      "\n",
      "Epoch 00018: LearningRateScheduler reducing learning rate to 0.0010289824943092664.\n",
      "Epoch 18/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0956 - custom_layer_accuracy: 0.9717 - custom_layer_1_accuracy: 0.9712Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  60.701044826423995  Class Avg =  60.65412597968545\n",
      "276/276 [==============================] - 102s 369ms/step - loss: 0.0956 - custom_layer_accuracy: 0.9717 - custom_layer_1_accuracy: 0.9712\n",
      "\n",
      "Epoch 00019: LearningRateScheduler reducing learning rate to 0.000989050578375961.\n",
      "Epoch 19/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0841 - custom_layer_accuracy: 0.9735 - custom_layer_1_accuracy: 0.9734Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  60.80215706100438  Class Avg =  60.72244200783295\n",
      "276/276 [==============================] - 102s 368ms/step - loss: 0.0841 - custom_layer_accuracy: 0.9735 - custom_layer_1_accuracy: 0.9734\n",
      "\n",
      "Epoch 00020: LearningRateScheduler reducing learning rate to 0.0009662250313680936.\n",
      "Epoch 20/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0813 - custom_layer_accuracy: 0.9755 - custom_layer_1_accuracy: 0.9770Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  60.53252443545669  Class Avg =  60.53837087150269\n",
      "276/276 [==============================] - 102s 369ms/step - loss: 0.0813 - custom_layer_accuracy: 0.9755 - custom_layer_1_accuracy: 0.9770\n",
      "\n",
      "Epoch 00021: LearningRateScheduler reducing learning rate to 9.990401707492283e-05.\n",
      "Epoch 21/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0707 - custom_layer_accuracy: 0.9790 - custom_layer_1_accuracy: 0.9814Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  60.56622851365015  Class Avg =  60.52932325246501\n",
      "276/276 [==============================] - 102s 369ms/step - loss: 0.0707 - custom_layer_accuracy: 0.9790 - custom_layer_1_accuracy: 0.9814\n",
      "\n",
      "Epoch 00022: LearningRateScheduler reducing learning rate to 0.00010036750350425956.\n",
      "Epoch 22/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0666 - custom_layer_accuracy: 0.9806 - custom_layer_1_accuracy: 0.9815Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  60.835861139197846  Class Avg =  60.79711986258968\n",
      "276/276 [==============================] - 102s 369ms/step - loss: 0.0666 - custom_layer_accuracy: 0.9806 - custom_layer_1_accuracy: 0.9815\n",
      "\n",
      "Epoch 00023: LearningRateScheduler reducing learning rate to 0.00010002878677666824.\n",
      "Epoch 23/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0617 - custom_layer_accuracy: 0.9831 - custom_layer_1_accuracy: 0.9844Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  60.80215706100438  Class Avg =  60.75146633799828\n",
      "276/276 [==============================] - 102s 369ms/step - loss: 0.0617 - custom_layer_accuracy: 0.9831 - custom_layer_1_accuracy: 0.9844\n",
      "\n",
      "Epoch 00024: LearningRateScheduler reducing learning rate to 0.00010372418964966554.\n",
      "Epoch 24/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0585 - custom_layer_accuracy: 0.9833 - custom_layer_1_accuracy: 0.9823Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  60.80215706100438  Class Avg =  60.74480393527169\n",
      "276/276 [==============================] - 102s 369ms/step - loss: 0.0585 - custom_layer_accuracy: 0.9833 - custom_layer_1_accuracy: 0.9823\n",
      "\n",
      "Epoch 00025: LearningRateScheduler reducing learning rate to 9.968604109466105e-05.\n",
      "Epoch 25/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0609 - custom_layer_accuracy: 0.9816 - custom_layer_1_accuracy: 0.9825Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  61.206605999325916  Class Avg =  61.141405576822436\n",
      "276/276 [==============================] - 102s 369ms/step - loss: 0.0609 - custom_layer_accuracy: 0.9816 - custom_layer_1_accuracy: 0.9825\n",
      "\n",
      "Epoch 00026: LearningRateScheduler reducing learning rate to 9.938184911214921e-05.\n",
      "Epoch 26/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0541 - custom_layer_accuracy: 0.9855 - custom_layer_1_accuracy: 0.9846Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  61.408830468486684  Class Avg =  61.346050680111745\n",
      "276/276 [==============================] - 102s 369ms/step - loss: 0.0541 - custom_layer_accuracy: 0.9855 - custom_layer_1_accuracy: 0.9846\n",
      "\n",
      "Epoch 00027: LearningRateScheduler reducing learning rate to 9.76139109520965e-05.\n",
      "Epoch 27/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0566 - custom_layer_accuracy: 0.9847 - custom_layer_1_accuracy: 0.9831Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  61.27401415571284  Class Avg =  61.2169987971471\n",
      "276/276 [==============================] - 102s 369ms/step - loss: 0.0566 - custom_layer_accuracy: 0.9847 - custom_layer_1_accuracy: 0.9831\n",
      "\n",
      "Epoch 00028: LearningRateScheduler reducing learning rate to 9.92531493143659e-05.\n",
      "Epoch 28/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0526 - custom_layer_accuracy: 0.9858 - custom_layer_1_accuracy: 0.9883Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  61.341422312099766  Class Avg =  61.28661138793401\n",
      "276/276 [==============================] - 102s 369ms/step - loss: 0.0526 - custom_layer_accuracy: 0.9858 - custom_layer_1_accuracy: 0.9883\n",
      "\n",
      "Epoch 00029: LearningRateScheduler reducing learning rate to 9.925039871595508e-05.\n",
      "Epoch 29/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0579 - custom_layer_accuracy: 0.9820 - custom_layer_1_accuracy: 0.9833Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  61.24031007751938  Class Avg =  61.18604641619946\n",
      "276/276 [==============================] - 102s 368ms/step - loss: 0.0579 - custom_layer_accuracy: 0.9820 - custom_layer_1_accuracy: 0.9833\n",
      "\n",
      "Epoch 00030: LearningRateScheduler reducing learning rate to 0.0001006694422490833.\n",
      "Epoch 30/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0557 - custom_layer_accuracy: 0.9848 - custom_layer_1_accuracy: 0.9855Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  61.10549376474553  Class Avg =  61.044351500970464\n",
      "276/276 [==============================] - 102s 368ms/step - loss: 0.0557 - custom_layer_accuracy: 0.9848 - custom_layer_1_accuracy: 0.9855\n",
      "\n",
      "Epoch 00031: LearningRateScheduler reducing learning rate to 0.00010302087411170678.\n",
      "Epoch 31/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0546 - custom_layer_accuracy: 0.9853 - custom_layer_1_accuracy: 0.9844Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  61.17290192113246  Class Avg =  61.094864696329545\n",
      "276/276 [==============================] - 102s 369ms/step - loss: 0.0546 - custom_layer_accuracy: 0.9853 - custom_layer_1_accuracy: 0.9844\n",
      "\n",
      "Epoch 00032: LearningRateScheduler reducing learning rate to 0.00010043625693444496.\n",
      "Epoch 32/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0546 - custom_layer_accuracy: 0.9847 - custom_layer_1_accuracy: 0.9861Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  61.10549376474553  Class Avg =  61.033408104747856\n",
      "276/276 [==============================] - 102s 368ms/step - loss: 0.0546 - custom_layer_accuracy: 0.9847 - custom_layer_1_accuracy: 0.9861\n",
      "\n",
      "Epoch 00033: LearningRateScheduler reducing learning rate to 9.822960229969011e-05.\n",
      "Epoch 33/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0541 - custom_layer_accuracy: 0.9865 - custom_layer_1_accuracy: 0.9857Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  61.10549376474553  Class Avg =  61.040962127313584\n",
      "276/276 [==============================] - 102s 368ms/step - loss: 0.0541 - custom_layer_accuracy: 0.9865 - custom_layer_1_accuracy: 0.9857\n",
      "\n",
      "Epoch 00034: LearningRateScheduler reducing learning rate to 0.0001011460379003738.\n",
      "Epoch 34/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0451 - custom_layer_accuracy: 0.9888 - custom_layer_1_accuracy: 0.9887Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  61.038085608358614  Class Avg =  60.97182999094047\n",
      "276/276 [==============================] - 102s 370ms/step - loss: 0.0451 - custom_layer_accuracy: 0.9888 - custom_layer_1_accuracy: 0.9887\n",
      "\n",
      "Epoch 00035: LearningRateScheduler reducing learning rate to 0.00010384816966120108.\n",
      "Epoch 35/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0525 - custom_layer_accuracy: 0.9867 - custom_layer_1_accuracy: 0.9868Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  60.73474890461746  Class Avg =  60.65159638812257\n",
      "276/276 [==============================] - 102s 368ms/step - loss: 0.0525 - custom_layer_accuracy: 0.9867 - custom_layer_1_accuracy: 0.9868\n",
      "\n",
      "Epoch 00036: LearningRateScheduler reducing learning rate to 0.00010195669225382337.\n",
      "Epoch 36/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0498 - custom_layer_accuracy: 0.9875 - custom_layer_1_accuracy: 0.9866Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  61.038085608358614  Class Avg =  60.97384729034442\n",
      "276/276 [==============================] - 102s 368ms/step - loss: 0.0498 - custom_layer_accuracy: 0.9875 - custom_layer_1_accuracy: 0.9866\n",
      "\n",
      "Epoch 00037: LearningRateScheduler reducing learning rate to 0.00010103943402781086.\n",
      "Epoch 37/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0476 - custom_layer_accuracy: 0.9880 - custom_layer_1_accuracy: 0.9871Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  61.10549376474553  Class Avg =  61.03102715236773\n",
      "276/276 [==============================] - 101s 368ms/step - loss: 0.0476 - custom_layer_accuracy: 0.9880 - custom_layer_1_accuracy: 0.9871\n",
      "\n",
      "Epoch 00038: LearningRateScheduler reducing learning rate to 9.905193488106683e-05.\n",
      "Epoch 38/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0493 - custom_layer_accuracy: 0.9856 - custom_layer_1_accuracy: 0.9872Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  61.10549376474553  Class Avg =  61.036680676967116\n",
      "276/276 [==============================] - 102s 368ms/step - loss: 0.0493 - custom_layer_accuracy: 0.9856 - custom_layer_1_accuracy: 0.9872\n",
      "\n",
      "Epoch 00039: LearningRateScheduler reducing learning rate to 9.988030716260436e-05.\n",
      "Epoch 39/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0526 - custom_layer_accuracy: 0.9848 - custom_layer_1_accuracy: 0.9861Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  61.27401415571284  Class Avg =  61.20448155104607\n",
      "276/276 [==============================] - 102s 368ms/step - loss: 0.0526 - custom_layer_accuracy: 0.9848 - custom_layer_1_accuracy: 0.9861\n",
      "\n",
      "Epoch 00040: LearningRateScheduler reducing learning rate to 9.856641965922463e-05.\n",
      "Epoch 40/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0459 - custom_layer_accuracy: 0.9880 - custom_layer_1_accuracy: 0.9886Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  61.206605999325916  Class Avg =  61.14189075202431\n",
      "276/276 [==============================] - 102s 369ms/step - loss: 0.0459 - custom_layer_accuracy: 0.9880 - custom_layer_1_accuracy: 0.9886\n",
      "\n",
      "Epoch 00041: LearningRateScheduler reducing learning rate to 9.973244118335236e-06.\n",
      "Epoch 41/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0497 - custom_layer_accuracy: 0.9850 - custom_layer_1_accuracy: 0.9851Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  61.341422312099766  Class Avg =  61.28718062362286\n",
      "276/276 [==============================] - 102s 368ms/step - loss: 0.0497 - custom_layer_accuracy: 0.9850 - custom_layer_1_accuracy: 0.9851\n",
      "\n",
      "Epoch 00042: LearningRateScheduler reducing learning rate to 1.0078110521881668e-05.\n",
      "Epoch 42/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0518 - custom_layer_accuracy: 0.9868 - custom_layer_1_accuracy: 0.9871Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  61.27401415571284  Class Avg =  61.20561149454852\n",
      "276/276 [==============================] - 102s 368ms/step - loss: 0.0518 - custom_layer_accuracy: 0.9868 - custom_layer_1_accuracy: 0.9871\n",
      "\n",
      "Epoch 00043: LearningRateScheduler reducing learning rate to 9.644540069128725e-06.\n",
      "Epoch 43/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0478 - custom_layer_accuracy: 0.9870 - custom_layer_1_accuracy: 0.9884Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  61.10549376474553  Class Avg =  61.04221740012889\n",
      "276/276 [==============================] - 102s 368ms/step - loss: 0.0478 - custom_layer_accuracy: 0.9870 - custom_layer_1_accuracy: 0.9884\n",
      "\n",
      "Epoch 00044: LearningRateScheduler reducing learning rate to 9.971783720401363e-06.\n",
      "Epoch 44/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0496 - custom_layer_accuracy: 0.9886 - custom_layer_1_accuracy: 0.9868Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  61.341422312099766  Class Avg =  61.27498149773449\n",
      "276/276 [==============================] - 102s 368ms/step - loss: 0.0496 - custom_layer_accuracy: 0.9886 - custom_layer_1_accuracy: 0.9868\n",
      "\n",
      "Epoch 00045: LearningRateScheduler reducing learning rate to 1.0130162259212205e-05.\n",
      "Epoch 45/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0467 - custom_layer_accuracy: 0.9875 - custom_layer_1_accuracy: 0.9878Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  61.24031007751938  Class Avg =  61.18548570838586\n",
      "276/276 [==============================] - 102s 369ms/step - loss: 0.0467 - custom_layer_accuracy: 0.9875 - custom_layer_1_accuracy: 0.9878\n",
      "\n",
      "Epoch 00046: LearningRateScheduler reducing learning rate to 1.0166096794198805e-05.\n",
      "Epoch 46/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0509 - custom_layer_accuracy: 0.9862 - custom_layer_1_accuracy: 0.9871Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  61.17290192113246  Class Avg =  61.11994898523275\n",
      "276/276 [==============================] - 102s 368ms/step - loss: 0.0509 - custom_layer_accuracy: 0.9862 - custom_layer_1_accuracy: 0.9871\n",
      "\n",
      "Epoch 00047: LearningRateScheduler reducing learning rate to 9.986620036182694e-06.\n",
      "Epoch 47/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0464 - custom_layer_accuracy: 0.9883 - custom_layer_1_accuracy: 0.9866Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  61.375126390293225  Class Avg =  61.31994898519942\n",
      "276/276 [==============================] - 102s 368ms/step - loss: 0.0464 - custom_layer_accuracy: 0.9883 - custom_layer_1_accuracy: 0.9866\n",
      "\n",
      "Epoch 00048: LearningRateScheduler reducing learning rate to 1.012925141345537e-05.\n",
      "Epoch 48/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0449 - custom_layer_accuracy: 0.9878 - custom_layer_1_accuracy: 0.9900Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  61.408830468486684  Class Avg =  61.34516332421024\n",
      "276/276 [==============================] - 102s 368ms/step - loss: 0.0449 - custom_layer_accuracy: 0.9878 - custom_layer_1_accuracy: 0.9900\n",
      "\n",
      "Epoch 00049: LearningRateScheduler reducing learning rate to 9.958986923344869e-06.\n",
      "Epoch 49/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0472 - custom_layer_accuracy: 0.9865 - custom_layer_1_accuracy: 0.9888Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  61.3077182339063  Class Avg =  61.25554646948638\n",
      "276/276 [==============================] - 102s 368ms/step - loss: 0.0472 - custom_layer_accuracy: 0.9865 - custom_layer_1_accuracy: 0.9888\n",
      "\n",
      "Epoch 00050: LearningRateScheduler reducing learning rate to 1.0401800882657558e-05.\n",
      "Epoch 50/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0490 - custom_layer_accuracy: 0.9879 - custom_layer_1_accuracy: 0.9881Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  61.206605999325916  Class Avg =  61.1483148310876\n",
      "276/276 [==============================] - 102s 368ms/step - loss: 0.0490 - custom_layer_accuracy: 0.9879 - custom_layer_1_accuracy: 0.9881\n",
      "\n",
      "Epoch 00051: LearningRateScheduler reducing learning rate to 1.0340586741165713e-05.\n",
      "Epoch 51/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0495 - custom_layer_accuracy: 0.9864 - custom_layer_1_accuracy: 0.9874Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  61.47623862487361  Class Avg =  61.418940563906006\n",
      "276/276 [==============================] - 102s 369ms/step - loss: 0.0495 - custom_layer_accuracy: 0.9864 - custom_layer_1_accuracy: 0.9874\n",
      "\n",
      "Epoch 00052: LearningRateScheduler reducing learning rate to 1.0244805407827896e-05.\n",
      "Epoch 52/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0476 - custom_layer_accuracy: 0.9849 - custom_layer_1_accuracy: 0.9870Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  61.47623862487361  Class Avg =  61.40912239039776\n",
      "276/276 [==============================] - 102s 369ms/step - loss: 0.0476 - custom_layer_accuracy: 0.9849 - custom_layer_1_accuracy: 0.9870\n",
      "\n",
      "Epoch 00053: LearningRateScheduler reducing learning rate to 9.80291842419041e-06.\n",
      "Epoch 53/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0437 - custom_layer_accuracy: 0.9881 - custom_layer_1_accuracy: 0.9897Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  61.139197842939  Class Avg =  61.079061629339236\n",
      "276/276 [==============================] - 102s 369ms/step - loss: 0.0437 - custom_layer_accuracy: 0.9881 - custom_layer_1_accuracy: 0.9897\n",
      "\n",
      "Epoch 00054: LearningRateScheduler reducing learning rate to 1.019693112608534e-05.\n",
      "Epoch 54/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0478 - custom_layer_accuracy: 0.9868 - custom_layer_1_accuracy: 0.9889Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  61.27401415571284  Class Avg =  61.21498149774315\n",
      "276/276 [==============================] - 102s 369ms/step - loss: 0.0478 - custom_layer_accuracy: 0.9868 - custom_layer_1_accuracy: 0.9889\n",
      "\n",
      "Epoch 00055: LearningRateScheduler reducing learning rate to 1.003838219171764e-05.\n",
      "Epoch 55/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0453 - custom_layer_accuracy: 0.9880 - custom_layer_1_accuracy: 0.9879Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  61.408830468486684  Class Avg =  61.352152375024744\n",
      "276/276 [==============================] - 102s 369ms/step - loss: 0.0453 - custom_layer_accuracy: 0.9880 - custom_layer_1_accuracy: 0.9879\n",
      "\n",
      "Epoch 00056: LearningRateScheduler reducing learning rate to 1.0219811847478392e-05.\n",
      "Epoch 56/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0450 - custom_layer_accuracy: 0.9890 - custom_layer_1_accuracy: 0.9887Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  61.17290192113246  Class Avg =  61.10561149456518\n",
      "276/276 [==============================] - 102s 369ms/step - loss: 0.0450 - custom_layer_accuracy: 0.9890 - custom_layer_1_accuracy: 0.9887\n",
      "\n",
      "Epoch 00057: LearningRateScheduler reducing learning rate to 9.99197095079294e-06.\n",
      "Epoch 57/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0486 - custom_layer_accuracy: 0.9854 - custom_layer_1_accuracy: 0.9871Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  61.408830468486684  Class Avg =  61.34153136295243\n",
      "276/276 [==============================] - 102s 369ms/step - loss: 0.0486 - custom_layer_accuracy: 0.9854 - custom_layer_1_accuracy: 0.9871\n",
      "\n",
      "Epoch 00058: LearningRateScheduler reducing learning rate to 9.977303324485918e-06.\n",
      "Epoch 58/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0441 - custom_layer_accuracy: 0.9891 - custom_layer_1_accuracy: 0.9883Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  61.341422312099766  Class Avg =  61.287749859311695\n",
      "276/276 [==============================] - 102s 369ms/step - loss: 0.0441 - custom_layer_accuracy: 0.9891 - custom_layer_1_accuracy: 0.9883\n",
      "\n",
      "Epoch 00059: LearningRateScheduler reducing learning rate to 1.015978674443001e-05.\n",
      "Epoch 59/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0481 - custom_layer_accuracy: 0.9865 - custom_layer_1_accuracy: 0.9884Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  61.10549376474553  Class Avg =  61.04977142269461\n",
      "276/276 [==============================] - 102s 369ms/step - loss: 0.0481 - custom_layer_accuracy: 0.9865 - custom_layer_1_accuracy: 0.9884\n",
      "\n",
      "Epoch 00060: LearningRateScheduler reducing learning rate to 1.0155552114939305e-05.\n",
      "Epoch 60/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0474 - custom_layer_accuracy: 0.9861 - custom_layer_1_accuracy: 0.9872Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  61.44253454668015  Class Avg =  61.380518220879594\n",
      "276/276 [==============================] - 102s 369ms/step - loss: 0.0474 - custom_layer_accuracy: 0.9861 - custom_layer_1_accuracy: 0.9872\n",
      "\n",
      "Epoch 00061: LearningRateScheduler reducing learning rate to 1.0047495146431887e-06.\n",
      "Epoch 61/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0459 - custom_layer_accuracy: 0.9874 - custom_layer_1_accuracy: 0.9871Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  61.3077182339063  Class Avg =  61.24391231534921\n",
      "276/276 [==============================] - 102s 369ms/step - loss: 0.0459 - custom_layer_accuracy: 0.9874 - custom_layer_1_accuracy: 0.9871\n",
      "\n",
      "Epoch 00062: LearningRateScheduler reducing learning rate to 9.8450742286806e-07.\n",
      "Epoch 62/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0457 - custom_layer_accuracy: 0.9866 - custom_layer_1_accuracy: 0.9879Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  61.408830468486684  Class Avg =  61.35182572693684\n",
      "276/276 [==============================] - 102s 369ms/step - loss: 0.0457 - custom_layer_accuracy: 0.9866 - custom_layer_1_accuracy: 0.9879\n",
      "\n",
      "Epoch 00063: LearningRateScheduler reducing learning rate to 1.0036197115566434e-06.\n",
      "Epoch 63/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0480 - custom_layer_accuracy: 0.9868 - custom_layer_1_accuracy: 0.9878Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  61.341422312099766  Class Avg =  61.286615651871635\n",
      "276/276 [==============================] - 102s 369ms/step - loss: 0.0480 - custom_layer_accuracy: 0.9868 - custom_layer_1_accuracy: 0.9878\n",
      "\n",
      "Epoch 00064: LearningRateScheduler reducing learning rate to 1.0147456373553651e-06.\n",
      "Epoch 64/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0497 - custom_layer_accuracy: 0.9854 - custom_layer_1_accuracy: 0.9866Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  61.341422312099766  Class Avg =  61.27611570517456\n",
      "276/276 [==============================] - 102s 368ms/step - loss: 0.0497 - custom_layer_accuracy: 0.9854 - custom_layer_1_accuracy: 0.9866\n",
      "\n",
      "Epoch 00065: LearningRateScheduler reducing learning rate to 9.943086451053936e-07.\n",
      "Epoch 65/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0487 - custom_layer_accuracy: 0.9862 - custom_layer_1_accuracy: 0.9876Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  61.27401415571284  Class Avg =  61.21611570518323\n",
      "276/276 [==============================] - 102s 369ms/step - loss: 0.0487 - custom_layer_accuracy: 0.9862 - custom_layer_1_accuracy: 0.9876\n",
      "\n",
      "Epoch 00066: LearningRateScheduler reducing learning rate to 9.944146220023559e-07.\n",
      "Epoch 66/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0483 - custom_layer_accuracy: 0.9868 - custom_layer_1_accuracy: 0.9861Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  61.17290192113246  Class Avg =  61.11001401028689\n",
      "276/276 [==============================] - 102s 369ms/step - loss: 0.0483 - custom_layer_accuracy: 0.9868 - custom_layer_1_accuracy: 0.9861\n",
      "\n",
      "Epoch 00067: LearningRateScheduler reducing learning rate to 9.852691096070494e-07.\n",
      "Epoch 67/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0496 - custom_layer_accuracy: 0.9859 - custom_layer_1_accuracy: 0.9866Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  61.10549376474553  Class Avg =  61.04007477141208\n",
      "276/276 [==============================] - 102s 369ms/step - loss: 0.0496 - custom_layer_accuracy: 0.9859 - custom_layer_1_accuracy: 0.9866\n",
      "\n",
      "Epoch 00068: LearningRateScheduler reducing learning rate to 9.73054330125336e-07.\n",
      "Epoch 68/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0446 - custom_layer_accuracy: 0.9899 - custom_layer_1_accuracy: 0.9900Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  61.341422312099766  Class Avg =  61.27950979961952\n",
      "276/276 [==============================] - 102s 369ms/step - loss: 0.0446 - custom_layer_accuracy: 0.9899 - custom_layer_1_accuracy: 0.9900\n",
      "\n",
      "Epoch 00069: LearningRateScheduler reducing learning rate to 1.0049276824791671e-06.\n",
      "Epoch 69/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0480 - custom_layer_accuracy: 0.9888 - custom_layer_1_accuracy: 0.9848Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  61.375126390293225  Class Avg =  61.314416525975254\n",
      "276/276 [==============================] - 102s 368ms/step - loss: 0.0480 - custom_layer_accuracy: 0.9888 - custom_layer_1_accuracy: 0.9848\n",
      "\n",
      "Epoch 00070: LearningRateScheduler reducing learning rate to 9.625875421378227e-07.\n",
      "Epoch 70/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0491 - custom_layer_accuracy: 0.9865 - custom_layer_1_accuracy: 0.9871Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  61.10549376474553  Class Avg =  61.042782371880115\n",
      "276/276 [==============================] - 102s 369ms/step - loss: 0.0491 - custom_layer_accuracy: 0.9865 - custom_layer_1_accuracy: 0.9871\n",
      "\n",
      "Epoch 00071: LearningRateScheduler reducing learning rate to 1.0030828296068528e-06.\n",
      "Epoch 71/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0431 - custom_layer_accuracy: 0.9883 - custom_layer_1_accuracy: 0.9892Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  61.341422312099766  Class Avg =  61.27611570517456\n",
      "276/276 [==============================] - 102s 369ms/step - loss: 0.0431 - custom_layer_accuracy: 0.9883 - custom_layer_1_accuracy: 0.9892\n",
      "\n",
      "Epoch 00072: LearningRateScheduler reducing learning rate to 9.634949877778995e-07.\n",
      "Epoch 72/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0510 - custom_layer_accuracy: 0.9834 - custom_layer_1_accuracy: 0.9857Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  61.27401415571284  Class Avg =  61.21566753486962\n",
      "276/276 [==============================] - 102s 368ms/step - loss: 0.0510 - custom_layer_accuracy: 0.9834 - custom_layer_1_accuracy: 0.9857\n",
      "\n",
      "Epoch 00073: LearningRateScheduler reducing learning rate to 1.0224208266985679e-06.\n",
      "Epoch 73/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0475 - custom_layer_accuracy: 0.9878 - custom_layer_1_accuracy: 0.9881Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  61.10549376474553  Class Avg =  61.04774985935305\n",
      "276/276 [==============================] - 102s 369ms/step - loss: 0.0475 - custom_layer_accuracy: 0.9878 - custom_layer_1_accuracy: 0.9881\n",
      "\n",
      "Epoch 00074: LearningRateScheduler reducing learning rate to 9.701886700161921e-07.\n",
      "Epoch 74/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0452 - custom_layer_accuracy: 0.9890 - custom_layer_1_accuracy: 0.9893Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  61.375126390293225  Class Avg =  61.31477591501382\n",
      "276/276 [==============================] - 102s 368ms/step - loss: 0.0452 - custom_layer_accuracy: 0.9890 - custom_layer_1_accuracy: 0.9893\n",
      "\n",
      "Epoch 00075: LearningRateScheduler reducing learning rate to 1.0023583202256484e-06.\n",
      "Epoch 75/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0458 - custom_layer_accuracy: 0.9874 - custom_layer_1_accuracy: 0.9887Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  61.341422312099766  Class Avg =  61.27611570517456\n",
      "276/276 [==============================] - 102s 368ms/step - loss: 0.0458 - custom_layer_accuracy: 0.9874 - custom_layer_1_accuracy: 0.9887\n",
      "\n",
      "Epoch 00076: LearningRateScheduler reducing learning rate to 1.0519475396595367e-06.\n",
      "Epoch 76/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0487 - custom_layer_accuracy: 0.9858 - custom_layer_1_accuracy: 0.9855Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  61.206605999325916  Class Avg =  61.14132578027309\n",
      "276/276 [==============================] - 102s 368ms/step - loss: 0.0487 - custom_layer_accuracy: 0.9858 - custom_layer_1_accuracy: 0.9855\n",
      "\n",
      "Epoch 00077: LearningRateScheduler reducing learning rate to 1.0153457621075006e-06.\n",
      "Epoch 77/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0496 - custom_layer_accuracy: 0.9862 - custom_layer_1_accuracy: 0.9858Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  61.44253454668015  Class Avg =  61.380518220879594\n",
      "276/276 [==============================] - 102s 370ms/step - loss: 0.0496 - custom_layer_accuracy: 0.9862 - custom_layer_1_accuracy: 0.9858\n",
      "\n",
      "Epoch 00078: LearningRateScheduler reducing learning rate to 9.652739700257876e-07.\n",
      "Epoch 78/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0451 - custom_layer_accuracy: 0.9882 - custom_layer_1_accuracy: 0.9871Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  61.408830468486684  Class Avg =  61.35747925153623\n",
      "276/276 [==============================] - 102s 368ms/step - loss: 0.0451 - custom_layer_accuracy: 0.9882 - custom_layer_1_accuracy: 0.9871\n",
      "\n",
      "Epoch 00079: LearningRateScheduler reducing learning rate to 1.0474478733637518e-06.\n",
      "Epoch 79/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0479 - custom_layer_accuracy: 0.9859 - custom_layer_1_accuracy: 0.9866Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  61.375126390293225  Class Avg =  61.320513956950634\n",
      "276/276 [==============================] - 102s 368ms/step - loss: 0.0479 - custom_layer_accuracy: 0.9859 - custom_layer_1_accuracy: 0.9866\n",
      "\n",
      "Epoch 00080: LearningRateScheduler reducing learning rate to 1.0003129454942954e-06.\n",
      "Epoch 80/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0472 - custom_layer_accuracy: 0.9871 - custom_layer_1_accuracy: 0.9888Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  61.139197842939  Class Avg =  61.080191572841684\n",
      "276/276 [==============================] - 102s 369ms/step - loss: 0.0472 - custom_layer_accuracy: 0.9871 - custom_layer_1_accuracy: 0.9888\n",
      "\n",
      "Epoch 00081: LearningRateScheduler reducing learning rate to 1.028144269201532e-07.\n",
      "Epoch 81/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0444 - custom_layer_accuracy: 0.9870 - custom_layer_1_accuracy: 0.9887Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  61.206605999325916  Class Avg =  61.1483148310876\n",
      "276/276 [==============================] - 102s 368ms/step - loss: 0.0444 - custom_layer_accuracy: 0.9870 - custom_layer_1_accuracy: 0.9887\n",
      "\n",
      "Epoch 00082: LearningRateScheduler reducing learning rate to 9.806085120497771e-08.\n",
      "Epoch 82/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0519 - custom_layer_accuracy: 0.9864 - custom_layer_1_accuracy: 0.9861Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  61.206605999325916  Class Avg =  61.14334734361467\n",
      "276/276 [==============================] - 102s 369ms/step - loss: 0.0519 - custom_layer_accuracy: 0.9864 - custom_layer_1_accuracy: 0.9861\n",
      "\n",
      "Epoch 00083: LearningRateScheduler reducing learning rate to 1.0187709593966903e-07.\n",
      "Epoch 83/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0465 - custom_layer_accuracy: 0.9878 - custom_layer_1_accuracy: 0.9876Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  61.24031007751938  Class Avg =  61.183912315357894\n",
      "276/276 [==============================] - 102s 369ms/step - loss: 0.0465 - custom_layer_accuracy: 0.9878 - custom_layer_1_accuracy: 0.9876\n",
      "\n",
      "Epoch 00084: LearningRateScheduler reducing learning rate to 1.0378539447165294e-07.\n",
      "Epoch 84/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0466 - custom_layer_accuracy: 0.9863 - custom_layer_1_accuracy: 0.9882Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  61.375126390293225  Class Avg =  61.31498149772649\n",
      "276/276 [==============================] - 102s 369ms/step - loss: 0.0466 - custom_layer_accuracy: 0.9863 - custom_layer_1_accuracy: 0.9882\n",
      "\n",
      "Epoch 00085: LearningRateScheduler reducing learning rate to 1.0074701207692291e-07.\n",
      "Epoch 85/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0481 - custom_layer_accuracy: 0.9861 - custom_layer_1_accuracy: 0.9867Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  61.408830468486684  Class Avg =  61.35271734677597\n",
      "276/276 [==============================] - 102s 369ms/step - loss: 0.0481 - custom_layer_accuracy: 0.9861 - custom_layer_1_accuracy: 0.9867\n",
      "\n",
      "Epoch 00086: LearningRateScheduler reducing learning rate to 1.0294960056862252e-07.\n",
      "Epoch 86/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0486 - custom_layer_accuracy: 0.9881 - custom_layer_1_accuracy: 0.9867Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  61.3077182339063  Class Avg =  61.246858239480574\n",
      "276/276 [==============================] - 102s 369ms/step - loss: 0.0486 - custom_layer_accuracy: 0.9881 - custom_layer_1_accuracy: 0.9867\n",
      "\n",
      "Epoch 00087: LearningRateScheduler reducing learning rate to 1.0067238439184563e-07.\n",
      "Epoch 87/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0460 - custom_layer_accuracy: 0.9893 - custom_layer_1_accuracy: 0.9888Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  61.408830468486684  Class Avg =  61.34447728708378\n",
      "276/276 [==============================] - 102s 369ms/step - loss: 0.0460 - custom_layer_accuracy: 0.9893 - custom_layer_1_accuracy: 0.9888\n",
      "\n",
      "Epoch 00088: LearningRateScheduler reducing learning rate to 1.0119096805849219e-07.\n",
      "Epoch 88/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0521 - custom_layer_accuracy: 0.9863 - custom_layer_1_accuracy: 0.9854Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  61.139197842939  Class Avg =  61.08334734362333\n",
      "276/276 [==============================] - 102s 370ms/step - loss: 0.0521 - custom_layer_accuracy: 0.9863 - custom_layer_1_accuracy: 0.9854\n",
      "\n",
      "Epoch 00089: LearningRateScheduler reducing learning rate to 9.954613289902393e-08.\n",
      "Epoch 89/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0488 - custom_layer_accuracy: 0.9865 - custom_layer_1_accuracy: 0.9856Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  61.24031007751938  Class Avg =  61.179267212035235\n",
      "276/276 [==============================] - 102s 368ms/step - loss: 0.0488 - custom_layer_accuracy: 0.9865 - custom_layer_1_accuracy: 0.9856\n",
      "\n",
      "Epoch 00090: LearningRateScheduler reducing learning rate to 1.0330162268108572e-07.\n",
      "Epoch 90/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0443 - custom_layer_accuracy: 0.9888 - custom_layer_1_accuracy: 0.9893Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  61.17290192113246  Class Avg =  61.113851554257366\n",
      "276/276 [==============================] - 102s 368ms/step - loss: 0.0443 - custom_layer_accuracy: 0.9888 - custom_layer_1_accuracy: 0.9893\n",
      "\n",
      "Epoch 00091: LearningRateScheduler reducing learning rate to 1.017488397771175e-07.\n",
      "Epoch 91/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0439 - custom_layer_accuracy: 0.9899 - custom_layer_1_accuracy: 0.9892Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  61.206605999325916  Class Avg =  61.14153136298576\n",
      "276/276 [==============================] - 102s 368ms/step - loss: 0.0439 - custom_layer_accuracy: 0.9899 - custom_layer_1_accuracy: 0.9892\n",
      "\n",
      "Epoch 00092: LearningRateScheduler reducing learning rate to 1.0382902190794811e-07.\n",
      "Epoch 92/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0491 - custom_layer_accuracy: 0.9865 - custom_layer_1_accuracy: 0.9879Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  61.24031007751938  Class Avg =  61.17171318946951\n",
      "276/276 [==============================] - 102s 368ms/step - loss: 0.0491 - custom_layer_accuracy: 0.9865 - custom_layer_1_accuracy: 0.9879\n",
      "\n",
      "Epoch 00093: LearningRateScheduler reducing learning rate to 9.80889767826869e-08.\n",
      "Epoch 93/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0445 - custom_layer_accuracy: 0.9874 - custom_layer_1_accuracy: 0.9888Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  61.17290192113246  Class Avg =  61.11001401028689\n",
      "276/276 [==============================] - 102s 368ms/step - loss: 0.0445 - custom_layer_accuracy: 0.9874 - custom_layer_1_accuracy: 0.9888\n",
      "\n",
      "Epoch 00094: LearningRateScheduler reducing learning rate to 9.750764274357315e-08.\n",
      "Epoch 94/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0461 - custom_layer_accuracy: 0.9866 - custom_layer_1_accuracy: 0.9898Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  61.206605999325916  Class Avg =  61.14391231536589\n",
      "276/276 [==============================] - 102s 368ms/step - loss: 0.0461 - custom_layer_accuracy: 0.9866 - custom_layer_1_accuracy: 0.9898\n",
      "\n",
      "Epoch 00095: LearningRateScheduler reducing learning rate to 1.0273408478702331e-07.\n",
      "Epoch 95/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0465 - custom_layer_accuracy: 0.9867 - custom_layer_1_accuracy: 0.9901Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  61.341422312099766  Class Avg =  61.28051822089626\n",
      "276/276 [==============================] - 102s 369ms/step - loss: 0.0465 - custom_layer_accuracy: 0.9867 - custom_layer_1_accuracy: 0.9901\n",
      "\n",
      "Epoch 00096: LearningRateScheduler reducing learning rate to 9.63311924323293e-08.\n",
      "Epoch 96/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0486 - custom_layer_accuracy: 0.9859 - custom_layer_1_accuracy: 0.9882Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  61.10549376474553  Class Avg =  61.04560723063622\n",
      "276/276 [==============================] - 102s 369ms/step - loss: 0.0486 - custom_layer_accuracy: 0.9859 - custom_layer_1_accuracy: 0.9882\n",
      "\n",
      "Epoch 00097: LearningRateScheduler reducing learning rate to 9.932582860688158e-08.\n",
      "Epoch 97/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0457 - custom_layer_accuracy: 0.9882 - custom_layer_1_accuracy: 0.9873Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  61.24031007751938  Class Avg =  61.18075654457624\n",
      "276/276 [==============================] - 102s 368ms/step - loss: 0.0457 - custom_layer_accuracy: 0.9882 - custom_layer_1_accuracy: 0.9873\n",
      "\n",
      "Epoch 00098: LearningRateScheduler reducing learning rate to 9.834743218167716e-08.\n",
      "Epoch 98/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0488 - custom_layer_accuracy: 0.9867 - custom_layer_1_accuracy: 0.9871Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  61.10549376474553  Class Avg =  61.04751153568972\n",
      "276/276 [==============================] - 102s 369ms/step - loss: 0.0488 - custom_layer_accuracy: 0.9867 - custom_layer_1_accuracy: 0.9871\n",
      "\n",
      "Epoch 00099: LearningRateScheduler reducing learning rate to 1.0360887795072904e-07.\n",
      "Epoch 99/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0470 - custom_layer_accuracy: 0.9873 - custom_layer_1_accuracy: 0.9880Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  61.206605999325916  Class Avg =  61.15114395378133\n",
      "276/276 [==============================] - 102s 369ms/step - loss: 0.0470 - custom_layer_accuracy: 0.9873 - custom_layer_1_accuracy: 0.9880\n",
      "\n",
      "Epoch 00100: LearningRateScheduler reducing learning rate to 1.0138547271802578e-07.\n",
      "Epoch 100/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0478 - custom_layer_accuracy: 0.9890 - custom_layer_1_accuracy: 0.9888Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  61.206605999325916  Class Avg =  61.1483148310876\n",
      "276/276 [==============================] - 102s 369ms/step - loss: 0.0478 - custom_layer_accuracy: 0.9890 - custom_layer_1_accuracy: 0.9888\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "892"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top1 = AverageMeter()\n",
    "class_avg = ClassAverageMeter(50)\n",
    "class CustomCallback(keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        top1.reset()\n",
    "        class_avg.reset(50)\n",
    "        predictions = model.predict((test_data, final_test_labels_cat, test_text_features, testing_phase))\n",
    "        print(\"Predictions Using Average result\")\n",
    "        avg_output = (predictions[0]+predictions[1])/2\n",
    "        prec1,class_acc,class_cnt,prec_prob = accuracy(avg_output, final_test_labels, 50)\n",
    "\n",
    "        top1.update(prec1, avg_output.shape[0])\n",
    "        class_avg.update(class_acc,class_cnt,prec_prob)\n",
    "\n",
    "        print(\"Epocs = \", i, \" Top1 = \", top1.avg, \" Class Avg = \", class_avg.avg)\n",
    "        del predictions\n",
    "        gc.collect()\n",
    "\n",
    "drop = 20\n",
    "def sgdr(epoch):\n",
    "    lr = learning_rate*(0.5 ** (epoch // drop))\n",
    "    return lr\n",
    "\n",
    "reduce_lr = tf.keras.callbacks.LearningRateScheduler(sgdr, verbose=True)\n",
    "on_epoch_callback = CustomCallback()\n",
    "\n",
    "training_phase = np.ones(shape=len(train_data), dtype=np.int64)\n",
    "final_train_labels_shuffled_cat = tf.keras.utils.to_categorical(final_train_labels_shuffled, num_classes=150)\n",
    "final_test_labels_cat = tf.keras.utils.to_categorical(final_test_labels, num_classes=50)\n",
    "\n",
    "model.fit((train_data_shuffled, final_train_labels_shuffled_cat, train_text_features_shuffled, training_phase),\n",
    "            final_train_labels_shuffled_cat, epochs=100, batch_size=32,\n",
    "            callbacks=[reduce_lr, on_epoch_callback])\n",
    "\n",
    "\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AREN with SE Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLayer_SE(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(CustomLayer, self).__init__()\n",
    "        self.shared_train_init = tf.constant_initializer(train_weights_matrix_numpy.transpose())\n",
    "        self.shared_train = Dense(len(train_weights_matrix_numpy), use_bias=False, kernel_initializer=self.shared_train_init, trainable=False, name=\"shared_train\")\n",
    "        self.shared_train.trainable = False\n",
    "        self.shared_test_init = tf.constant_initializer(test_weights_matrix_numpy.transpose())\n",
    "        self.shared_test = Dense(len(test_weights_matrix_numpy), use_bias=False, kernel_initializer=self.shared_test_init, trainable=False, name=\"shared_test\")\n",
    "        self.shared_test.trainable = False\n",
    "        \n",
    "    def train_step(self, are_features, acse_features):\n",
    "        return self.shared_train(are_features), self.shared_train(acse_features)\n",
    "    \n",
    "    def test_step(self, are_features, acse_features):\n",
    "        return self.shared_test(are_features), self.shared_test(acse_features)\n",
    "\n",
    "\n",
    "    def call(self, are_features, acse_features, is_training):\n",
    "        return tf.cond(tf.equal(is_training, 1),\n",
    "                       lambda: self.train_step(are_features, acse_features),\n",
    "                       lambda: self.test_step(are_features, acse_features))\n",
    "    \n",
    "class CustomPreprocessing_SE(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(CustomPreprocessing, self).__init__()\n",
    "        self.random_crop = tf.keras.layers.experimental.preprocessing.RandomCrop(224, 224)\n",
    "        self.random_flip = tf.keras.layers.experimental.preprocessing.RandomFlip('horizontal')\n",
    "        self.center_crop = tf.keras.layers.experimental.preprocessing.CenterCrop(224, 224)\n",
    "        self.mean_tensor = tf.constant([0.406, 0.456, 0.485], name=\"mean_tensor\")\n",
    "        self.std_tensor = tf.constant([0.225, 0.224, 0.229], name=\"std_tensor\")\n",
    "        self.preproc_layer = Lambda(lambda x: (tf.cast(x, tf.float32) - self.mean_tensor) / self.std_tensor)\n",
    "        \n",
    "    def train_preprocess(self, image_features):\n",
    "        image_features = self.random_crop(image_features)\n",
    "        image_features = self.random_flip(image_features)\n",
    "        image_features = self.preproc_layer(image_features)\n",
    "        return image_features\n",
    "    \n",
    "    def test_preprocess(self, image_features):\n",
    "        image_features = self.center_crop(image_features)\n",
    "        image_features = self.preproc_layer(image_features)\n",
    "        return image_features\n",
    "\n",
    "    def call(self, image_features, is_training):\n",
    "        return tf.cond(tf.equal(is_training, 1),\n",
    "                       lambda: self.train_preprocess(image_features),\n",
    "                       lambda: self.test_preprocess(image_features))\n",
    "\n",
    "class MyModel_SE(tf.keras.Model):\n",
    "    def __init__(self, k, alpha, cmps):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.resnet_base = ResNet101(include_top=False, input_shape=(224,224,3), weights=\"imagenet\")\n",
    "        self.resnet = Model(inputs=self.resnet_base.inputs, outputs=self.resnet_base.get_layer(\"conv5_block3_3_bn\").output)\n",
    "        self.flatten = Flatten()\n",
    "        self.custom_preprocess = CustomPreprocessing_SE()\n",
    "        self.conv_channels = 2048\n",
    "        self.threshold = alpha\n",
    "        self.parts = k\n",
    "        self.map_size = 7\n",
    "        self.pool2d = MaxPool2D(pool_size=self.map_size, strides=self.map_size)\n",
    "        self.conv2d = Conv2D(self.parts, 1, kernel_initializer=tf.keras.initializers.orthogonal)\n",
    "        self.resnet_features = None\n",
    "        self.resnet_feature_shapes = None\n",
    "        self.conv_bilinear = Conv2D(cmps, 1, kernel_initializer=tf.keras.initializers.orthogonal)\n",
    "        self.coef = self.map_size * self.map_size\n",
    "        self.p_linear = Dense(1024, use_bias=False, kernel_initializer=tf.keras.initializers.orthogonal)\n",
    "        self.b_linear = Dense(1024, use_bias=False, kernel_initializer=tf.keras.initializers.orthogonal)\n",
    "        self.attr_linear = Dense(1024, use_bias=False, kernel_initializer=tf.keras.initializers.orthogonal)\n",
    "        self.dropout_p = Dropout(0.4)\n",
    "        self.dropout_b = Dropout(0.4)\n",
    "        self.dropout_attr = Dropout(0.4)\n",
    "        self.batchnorm = BatchNormalization()\n",
    "        self.batchnorm_bilinear = BatchNormalization()\n",
    "        self.custom_layer = CustomLayer_SE()\n",
    "        self.shared_train_init = tf.constant_initializer(tf.keras.backend.l2_normalize(train_weights_matrix, axis=1).numpy())\n",
    "        self.dense_se_1 = Dense(512, use_bias=False, kernel_initializer=tf.keras.initializers.orthogonal)\n",
    "        self.dense_se_2 = Dense(2048, use_bias=False, kernel_initializer=tf.keras.initializers.orthogonal)\n",
    "        self.gap = GlobalAveragePooling2D()\n",
    "        \n",
    "\n",
    "    def call(self, input1, input2, input3):\n",
    "        input1 = self.custom_preprocess(input1, input3[0])\n",
    "\n",
    "        feat = self.resnet(input1)\n",
    "        feat = tf.keras.activations.relu(feat)\n",
    "        \n",
    "        # ==== Squeeze and Excitation Block ==== #\n",
    "        # ==== Squeeze Step ==== #\n",
    "        w = self.gap(feat)\n",
    "\n",
    "        # ==== Excitation Step ==== #\n",
    "        w = self.dense_se_1(w)\n",
    "        w = tf.keras.activations.relu(w)\n",
    "        w = self.dense_se_2(w)\n",
    "        w = tf.keras.activations.sigmoid(w)\n",
    "        w = tf.expand_dims(w, 1)\n",
    "        w = tf.expand_dims(w, 1)\n",
    "        w = tf.broadcast_to(w, [tf.shape(feat)[0], tf.shape(feat)[1], tf.shape(feat)[2], tf.shape(feat)[3]])\n",
    "        feat = tf.math.multiply(feat, w)\n",
    "\n",
    "        self.resnet_features = feat\n",
    "        self.resnet_feature_shapes = tf.shape(feat)\n",
    "        \n",
    "        # ==== Self Attention ==== #\n",
    "        att_weight = self.conv2d(feat)\n",
    "        att_weight = tf.keras.activations.sigmoid(att_weight)\n",
    "        \n",
    "        batch = tf.shape(att_weight)[0]\n",
    "        height = tf.convert_to_tensor(att_weight.shape[1])\n",
    "        width = tf.convert_to_tensor(att_weight.shape[2])\n",
    "        channels = tf.convert_to_tensor(att_weight.shape[3])\n",
    "        \n",
    "        global_max_value = tf.math.reduce_max(tf.reshape(att_weight, [batch, -1]), axis=1)\n",
    "        local_max_value = tf.math.reduce_max(tf.reshape(att_weight, [batch, -1, self.parts]), axis=1)\n",
    "\n",
    "        global_max_value = tf.broadcast_to(tf.expand_dims(global_max_value, axis=-1), shape=[batch, channels])\n",
    "        threshold_value = self.threshold * global_max_value\n",
    "\n",
    "        mask = tf.broadcast_to(tf.expand_dims(tf.expand_dims(tf.cast(tf.math.greater_equal(local_max_value, threshold_value), dtype=tf.float32), axis=1), axis=1),\n",
    "                               shape=[batch, height, width, self.parts])\n",
    "\n",
    "        attention_weights = tf.math.multiply(att_weight, mask)\n",
    "        attention_weights = tf.transpose(attention_weights, perm=[3, 0, 1, 2])\n",
    "        attention_weights = tf.expand_dims(attention_weights, axis=2)\n",
    "        attention_weights_shape = tf.shape(attention_weights)\n",
    "        attention_weights = tf.broadcast_to(attention_weights, shape=[attention_weights_shape[0], attention_weights_shape[1], self.conv_channels, attention_weights_shape[3], attention_weights_shape[4]])\n",
    "        attention_weights = tf.transpose(attention_weights, perm=[0, 1, 3, 4, 2])\n",
    "        features = tf.broadcast_to(tf.expand_dims(self.resnet_features, axis=0), [self.parts, self.resnet_feature_shapes[0], self.resnet_feature_shapes[1], self.resnet_feature_shapes[2], self.resnet_feature_shapes[3]])\n",
    "        Y = tf.math.multiply(attention_weights, features)\n",
    "        \n",
    "        # ==== ARE Network ==== #\n",
    "        are_output = tf.map_fn(lambda vec: self.pool2d(vec), Y)\n",
    "        are_output = tf.squeeze(are_output, [2,3])\n",
    "        are_output = tf.transpose(are_output, [1,0,2])\n",
    "        weights_batch = tf.shape(are_output)[0]\n",
    "        weights_channel = are_output.shape[1]\n",
    "        weights_h = are_output.shape[2]\n",
    "        are_output = tf.reshape(are_output, (weights_batch, weights_channel*weights_h))\n",
    "\n",
    "\n",
    "        # ==== ACSE Network ==== #\n",
    "        \n",
    "        Y_shape = tf.shape(Y)\n",
    "        Y_temp = tf.reshape(Y, shape=[Y.shape[0], Y_shape[1], Y.shape[2]*Y.shape[3], Y.shape[4]])\n",
    "        self.bilinear = self.conv_bilinear(self.resnet_features)\n",
    "        batch = tf.shape(self.bilinear)[0]\n",
    "        h = self.bilinear.shape[1]\n",
    "        w = self.bilinear.shape[2]\n",
    "        channels = self.bilinear.shape[3]\n",
    "        X = tf.reshape(self.bilinear, shape=[batch, h*w, channels])\n",
    "        X = tf.transpose(X, [0,2,1])\n",
    "        acse_output = tf.map_fn(lambda vec: (tf.einsum('ikj,ijl -> ikl', X, vec) / self.coef), Y_temp)\n",
    "        acse_output_shape = tf.shape(acse_output)\n",
    "        acse_output = tf.reshape(acse_output, shape=[acse_output.shape[0], acse_output_shape[1],\n",
    "                                                     acse_output.shape[2]*acse_output.shape[3]])\n",
    "        acse_output = tf.math.reduce_max(acse_output, axis=0)\n",
    "\n",
    "        are_feat = self.dropout_p(self.p_linear(are_output))\n",
    "        acse_feat = self.dropout_b(self.b_linear(acse_output))\n",
    "        \n",
    "        are_feat, acse_feat = self.custom_layer(are_feat, acse_feat, input3[0])\n",
    " \n",
    "        return are_feat, acse_feat\n",
    "\n",
    "    def model(self):\n",
    "        inp = Input(shape=(256,256,3))\n",
    "        inp1 = Input(shape=(150))\n",
    "        inp2 = Input(shape=(312))\n",
    "        inp3 = Input(shape=())\n",
    "        return Model(inputs=(inp, inp1, inp2, inp3), outputs=self.call(inp, inp2, inp3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_obj = MyModel_SE(12, 1, 20)\n",
    "model = model_obj.model()\n",
    "learning_rate = 0.001\n",
    "\n",
    "cls_loss = (classification_loss(model.outputs[0], model.inputs[1]) +\n",
    "                classification_loss(model.outputs[1], model.inputs[1]))/2\n",
    "model.add_loss(cls_loss)\n",
    "\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate, momentum=0.9, decay=0.0005)\n",
    "model.compile(optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "\n",
    "for layer in model.layers:\n",
    "    if \"custom_layer\" in layer.name:\n",
    "        layer.trainable = False\n",
    "        print(layer.trainable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00001: LearningRateScheduler reducing learning rate to 0.0010072777357323964.\n",
      "Epoch 1/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 3.6017 - custom_layer_1_accuracy: 0.2138 - custom_layer_1_1_accuracy: 0.1263Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  41.15267947421638  Class Avg =  41.06469321799734\n",
      "276/276 [==============================] - 104s 377ms/step - loss: 3.6017 - custom_layer_1_accuracy: 0.2138 - custom_layer_1_1_accuracy: 0.1263\n",
      "\n",
      "Epoch 00002: LearningRateScheduler reducing learning rate to 0.0009412323066747734.\n",
      "Epoch 2/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 1.7890 - custom_layer_1_accuracy: 0.4960 - custom_layer_1_1_accuracy: 0.4765Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  51.93798449612403  Class Avg =  52.06518813744715\n",
      "276/276 [==============================] - 101s 366ms/step - loss: 1.7890 - custom_layer_1_accuracy: 0.4960 - custom_layer_1_1_accuracy: 0.4765\n",
      "\n",
      "Epoch 00003: LearningRateScheduler reducing learning rate to 0.0010020418069368086.\n",
      "Epoch 3/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 1.2210 - custom_layer_1_accuracy: 0.6394 - custom_layer_1_1_accuracy: 0.6240Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  52.34243343444557  Class Avg =  52.40670260722156\n",
      "276/276 [==============================] - 101s 366ms/step - loss: 1.2210 - custom_layer_1_accuracy: 0.6394 - custom_layer_1_1_accuracy: 0.6240\n",
      "\n",
      "Epoch 00004: LearningRateScheduler reducing learning rate to 0.0010004986018021169.\n",
      "Epoch 4/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.8892 - custom_layer_1_accuracy: 0.7261 - custom_layer_1_1_accuracy: 0.7136Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  57.802494101786316  Class Avg =  57.88987740200824\n",
      "276/276 [==============================] - 101s 366ms/step - loss: 0.8892 - custom_layer_1_accuracy: 0.7261 - custom_layer_1_1_accuracy: 0.7136\n",
      "\n",
      "Epoch 00005: LearningRateScheduler reducing learning rate to 0.001001394829339664.\n",
      "Epoch 5/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.7222 - custom_layer_1_accuracy: 0.7744 - custom_layer_1_1_accuracy: 0.7673Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  57.60026963262555  Class Avg =  57.66715458373398\n",
      "276/276 [==============================] - 101s 366ms/step - loss: 0.7222 - custom_layer_1_accuracy: 0.7744 - custom_layer_1_1_accuracy: 0.7673\n",
      "\n",
      "Epoch 00006: LearningRateScheduler reducing learning rate to 0.000995565163425771.\n",
      "Epoch 6/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.5869 - custom_layer_1_accuracy: 0.8156 - custom_layer_1_1_accuracy: 0.8132Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  54.60060667340748  Class Avg =  54.67089573276073\n",
      "276/276 [==============================] - 101s 366ms/step - loss: 0.5869 - custom_layer_1_accuracy: 0.8156 - custom_layer_1_1_accuracy: 0.8132\n",
      "\n",
      "Epoch 00007: LearningRateScheduler reducing learning rate to 0.0010121207805933127.\n",
      "Epoch 7/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.4954 - custom_layer_1_accuracy: 0.8407 - custom_layer_1_1_accuracy: 0.8397Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  57.22952477249747  Class Avg =  57.34403871127086\n",
      "276/276 [==============================] - 101s 367ms/step - loss: 0.4954 - custom_layer_1_accuracy: 0.8407 - custom_layer_1_1_accuracy: 0.8397\n",
      "\n",
      "Epoch 00008: LearningRateScheduler reducing learning rate to 0.0009546322496227537.\n",
      "Epoch 8/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.4032 - custom_layer_1_accuracy: 0.8687 - custom_layer_1_1_accuracy: 0.8676Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  57.66767778901247  Class Avg =  57.69894391475439\n",
      "276/276 [==============================] - 101s 366ms/step - loss: 0.4032 - custom_layer_1_accuracy: 0.8687 - custom_layer_1_1_accuracy: 0.8676\n",
      "\n",
      "Epoch 00009: LearningRateScheduler reducing learning rate to 0.0009706678665840208.\n",
      "Epoch 9/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.3448 - custom_layer_1_accuracy: 0.8861 - custom_layer_1_1_accuracy: 0.8858Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  59.150657229524775  Class Avg =  59.23400261977085\n",
      "276/276 [==============================] - 101s 366ms/step - loss: 0.3448 - custom_layer_1_accuracy: 0.8861 - custom_layer_1_1_accuracy: 0.8858\n",
      "\n",
      "Epoch 00010: LearningRateScheduler reducing learning rate to 0.0009825358356542204.\n",
      "Epoch 10/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.3064 - custom_layer_1_accuracy: 0.9010 - custom_layer_1_1_accuracy: 0.8987Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  58.27435119649478  Class Avg =  58.35243638421711\n",
      "276/276 [==============================] - 101s 366ms/step - loss: 0.3064 - custom_layer_1_accuracy: 0.9010 - custom_layer_1_1_accuracy: 0.8987\n",
      "\n",
      "Epoch 00011: LearningRateScheduler reducing learning rate to 0.0010627868999822036.\n",
      "Epoch 11/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.2508 - custom_layer_1_accuracy: 0.9189 - custom_layer_1_1_accuracy: 0.9181Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  58.476575665655545  Class Avg =  58.388443043756936\n",
      "276/276 [==============================] - 101s 365ms/step - loss: 0.2508 - custom_layer_1_accuracy: 0.9189 - custom_layer_1_1_accuracy: 0.9181\n",
      "\n",
      "Epoch 00012: LearningRateScheduler reducing learning rate to 0.0009939882284080704.\n",
      "Epoch 12/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.2336 - custom_layer_1_accuracy: 0.9230 - custom_layer_1_1_accuracy: 0.9211Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  57.70138186720593  Class Avg =  57.71333393272944\n",
      "276/276 [==============================] - 101s 366ms/step - loss: 0.2336 - custom_layer_1_accuracy: 0.9230 - custom_layer_1_1_accuracy: 0.9211\n",
      "\n",
      "Epoch 00013: LearningRateScheduler reducing learning rate to 0.0010248583007057234.\n",
      "Epoch 13/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.2073 - custom_layer_1_accuracy: 0.9342 - custom_layer_1_1_accuracy: 0.9354Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  60.66734074823054  Class Avg =  60.74730609478313\n",
      "276/276 [==============================] - 101s 366ms/step - loss: 0.2073 - custom_layer_1_accuracy: 0.9342 - custom_layer_1_1_accuracy: 0.9354\n",
      "\n",
      "Epoch 00014: LearningRateScheduler reducing learning rate to 0.0009819276874630726.\n",
      "Epoch 14/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.1761 - custom_layer_1_accuracy: 0.9433 - custom_layer_1_1_accuracy: 0.9420Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  59.75733063700708  Class Avg =  59.82871366174857\n",
      "276/276 [==============================] - 101s 366ms/step - loss: 0.1761 - custom_layer_1_accuracy: 0.9433 - custom_layer_1_1_accuracy: 0.9420\n",
      "\n",
      "Epoch 00015: LearningRateScheduler reducing learning rate to 0.0010011579484813541.\n",
      "Epoch 15/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.1637 - custom_layer_1_accuracy: 0.9481 - custom_layer_1_1_accuracy: 0.9480Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  59.824738793394  Class Avg =  59.86421338002475\n",
      "276/276 [==============================] - 101s 366ms/step - loss: 0.1637 - custom_layer_1_accuracy: 0.9481 - custom_layer_1_1_accuracy: 0.9480\n",
      "\n",
      "Epoch 00016: LearningRateScheduler reducing learning rate to 0.0009950786752692935.\n",
      "Epoch 16/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.1485 - custom_layer_1_accuracy: 0.9544 - custom_layer_1_1_accuracy: 0.9544Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  58.982136838557466  Class Avg =  58.93670350934008\n",
      "276/276 [==============================] - 101s 366ms/step - loss: 0.1485 - custom_layer_1_accuracy: 0.9544 - custom_layer_1_1_accuracy: 0.9544\n",
      "\n",
      "Epoch 00017: LearningRateScheduler reducing learning rate to 0.0009778285050366263.\n",
      "Epoch 17/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.1390 - custom_layer_1_accuracy: 0.9556 - custom_layer_1_1_accuracy: 0.9583Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  59.99325918436131  Class Avg =  60.059163344357955\n",
      "276/276 [==============================] - 101s 367ms/step - loss: 0.1390 - custom_layer_1_accuracy: 0.9556 - custom_layer_1_1_accuracy: 0.9583\n",
      "\n",
      "Epoch 00018: LearningRateScheduler reducing learning rate to 0.0009921692947981449.\n",
      "Epoch 18/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.1215 - custom_layer_1_accuracy: 0.9616 - custom_layer_1_1_accuracy: 0.9643Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  59.521402089652845  Class Avg =  59.58660529685889\n",
      "276/276 [==============================] - 101s 366ms/step - loss: 0.1215 - custom_layer_1_accuracy: 0.9616 - custom_layer_1_1_accuracy: 0.9643\n",
      "\n",
      "Epoch 00019: LearningRateScheduler reducing learning rate to 0.001010562731168984.\n",
      "Epoch 19/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.1148 - custom_layer_1_accuracy: 0.9655 - custom_layer_1_1_accuracy: 0.9667Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  59.08324907313785  Class Avg =  59.12985364558269\n",
      "276/276 [==============================] - 101s 367ms/step - loss: 0.1148 - custom_layer_1_accuracy: 0.9655 - custom_layer_1_1_accuracy: 0.9667\n",
      "\n",
      "Epoch 00020: LearningRateScheduler reducing learning rate to 0.0010122674441146095.\n",
      "Epoch 20/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.1050 - custom_layer_1_accuracy: 0.9659 - custom_layer_1_1_accuracy: 0.9702Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  59.75733063700708  Class Avg =  59.797659250869195\n",
      "276/276 [==============================] - 101s 365ms/step - loss: 0.1050 - custom_layer_1_accuracy: 0.9659 - custom_layer_1_1_accuracy: 0.9702\n",
      "\n",
      "Epoch 00021: LearningRateScheduler reducing learning rate to 0.00010344076813410144.\n",
      "Epoch 21/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0925 - custom_layer_1_accuracy: 0.9718 - custom_layer_1_1_accuracy: 0.9736Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  60.46511627906977  Class Avg =  60.5060165687328\n",
      "276/276 [==============================] - 101s 366ms/step - loss: 0.0925 - custom_layer_1_accuracy: 0.9718 - custom_layer_1_1_accuracy: 0.9736\n",
      "\n",
      "Epoch 00022: LearningRateScheduler reducing learning rate to 9.961748341083443e-05.\n",
      "Epoch 22/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0883 - custom_layer_1_accuracy: 0.9765 - custom_layer_1_1_accuracy: 0.9755Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  60.53252443545669  Class Avg =  60.5845557131888\n",
      "276/276 [==============================] - 101s 366ms/step - loss: 0.0883 - custom_layer_1_accuracy: 0.9765 - custom_layer_1_1_accuracy: 0.9755\n",
      "\n",
      "Epoch 00023: LearningRateScheduler reducing learning rate to 0.00010198854419513158.\n",
      "Epoch 23/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0874 - custom_layer_1_accuracy: 0.9764 - custom_layer_1_1_accuracy: 0.9756Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  60.262891809909  Class Avg =  60.31009243639992\n",
      "276/276 [==============================] - 101s 366ms/step - loss: 0.0874 - custom_layer_1_accuracy: 0.9764 - custom_layer_1_1_accuracy: 0.9756\n",
      "\n",
      "Epoch 00024: LearningRateScheduler reducing learning rate to 0.00010258920652454804.\n",
      "Epoch 24/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0806 - custom_layer_1_accuracy: 0.9761 - custom_layer_1_1_accuracy: 0.9802Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  60.195483653522075  Class Avg =  60.236763367017765\n",
      "276/276 [==============================] - 101s 366ms/step - loss: 0.0806 - custom_layer_1_accuracy: 0.9761 - custom_layer_1_1_accuracy: 0.9802\n",
      "\n",
      "Epoch 00025: LearningRateScheduler reducing learning rate to 0.00010026736309422308.\n",
      "Epoch 25/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0851 - custom_layer_1_accuracy: 0.9748 - custom_layer_1_1_accuracy: 0.9790Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  60.43141220087631  Class Avg =  60.46896249287213\n",
      "276/276 [==============================] - 101s 366ms/step - loss: 0.0851 - custom_layer_1_accuracy: 0.9748 - custom_layer_1_1_accuracy: 0.9790\n",
      "\n",
      "Epoch 00026: LearningRateScheduler reducing learning rate to 9.756101926005912e-05.\n",
      "Epoch 26/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0793 - custom_layer_1_accuracy: 0.9766 - custom_layer_1_1_accuracy: 0.9805Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  60.262891809909  Class Avg =  60.294709062716876\n",
      "276/276 [==============================] - 101s 366ms/step - loss: 0.0793 - custom_layer_1_accuracy: 0.9766 - custom_layer_1_1_accuracy: 0.9805\n",
      "\n",
      "Epoch 00027: LearningRateScheduler reducing learning rate to 0.00010239208624039994.\n",
      "Epoch 27/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0788 - custom_layer_1_accuracy: 0.9802 - custom_layer_1_1_accuracy: 0.9793Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  60.43141220087631  Class Avg =  60.45709001507167\n",
      "276/276 [==============================] - 101s 366ms/step - loss: 0.0788 - custom_layer_1_accuracy: 0.9802 - custom_layer_1_1_accuracy: 0.9793\n",
      "\n",
      "Epoch 00028: LearningRateScheduler reducing learning rate to 0.00010230557067594381.\n",
      "Epoch 28/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0794 - custom_layer_1_accuracy: 0.9777 - custom_layer_1_1_accuracy: 0.9797Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  60.195483653522075  Class Avg =  60.220689692270646\n",
      "276/276 [==============================] - 101s 366ms/step - loss: 0.0794 - custom_layer_1_accuracy: 0.9777 - custom_layer_1_1_accuracy: 0.9797\n",
      "\n",
      "Epoch 00029: LearningRateScheduler reducing learning rate to 0.00010088088014476914.\n",
      "Epoch 29/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0756 - custom_layer_1_accuracy: 0.9800 - custom_layer_1_1_accuracy: 0.9803Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  60.39770812268284  Class Avg =  60.42238460749097\n",
      "276/276 [==============================] - 101s 366ms/step - loss: 0.0756 - custom_layer_1_accuracy: 0.9800 - custom_layer_1_1_accuracy: 0.9803\n",
      "\n",
      "Epoch 00030: LearningRateScheduler reducing learning rate to 0.00010164263801135695.\n",
      "Epoch 30/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0785 - custom_layer_1_accuracy: 0.9774 - custom_layer_1_1_accuracy: 0.9776Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  60.22918773171554  Class Avg =  60.25277201672074\n",
      "276/276 [==============================] - 101s 365ms/step - loss: 0.0785 - custom_layer_1_accuracy: 0.9774 - custom_layer_1_1_accuracy: 0.9776\n",
      "\n",
      "Epoch 00031: LearningRateScheduler reducing learning rate to 9.875552759226896e-05.\n",
      "Epoch 31/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0740 - custom_layer_1_accuracy: 0.9790 - custom_layer_1_1_accuracy: 0.9810Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  60.43141220087631  Class Avg =  60.45955974872686\n",
      "276/276 [==============================] - 101s 366ms/step - loss: 0.0740 - custom_layer_1_accuracy: 0.9790 - custom_layer_1_1_accuracy: 0.9810\n",
      "\n",
      "Epoch 00032: LearningRateScheduler reducing learning rate to 0.00010109541612431459.\n",
      "Epoch 32/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0711 - custom_layer_1_accuracy: 0.9807 - custom_layer_1_1_accuracy: 0.9827Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  60.43141220087631  Class Avg =  60.4623888714206\n",
      "276/276 [==============================] - 101s 366ms/step - loss: 0.0711 - custom_layer_1_accuracy: 0.9807 - custom_layer_1_1_accuracy: 0.9827\n",
      "\n",
      "Epoch 00033: LearningRateScheduler reducing learning rate to 0.0001019096307065968.\n",
      "Epoch 33/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0691 - custom_layer_1_accuracy: 0.9791 - custom_layer_1_1_accuracy: 0.9821Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  60.29659588810246  Class Avg =  60.321258927942814\n",
      "276/276 [==============================] - 101s 366ms/step - loss: 0.0691 - custom_layer_1_accuracy: 0.9791 - custom_layer_1_1_accuracy: 0.9821\n",
      "\n",
      "Epoch 00034: LearningRateScheduler reducing learning rate to 9.958770306362995e-05.\n",
      "Epoch 34/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0707 - custom_layer_1_accuracy: 0.9793 - custom_layer_1_1_accuracy: 0.9816Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  60.29659588810246  Class Avg =  60.32408805063656\n",
      "276/276 [==============================] - 101s 366ms/step - loss: 0.0707 - custom_layer_1_accuracy: 0.9793 - custom_layer_1_1_accuracy: 0.9816\n",
      "\n",
      "Epoch 00035: LearningRateScheduler reducing learning rate to 9.879213790835708e-05.\n",
      "Epoch 35/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0703 - custom_layer_1_accuracy: 0.9803 - custom_layer_1_1_accuracy: 0.9796Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  60.195483653522075  Class Avg =  60.235722204790385\n",
      "276/276 [==============================] - 101s 366ms/step - loss: 0.0703 - custom_layer_1_accuracy: 0.9803 - custom_layer_1_1_accuracy: 0.9796\n",
      "\n",
      "Epoch 00036: LearningRateScheduler reducing learning rate to 0.0001006674294174861.\n",
      "Epoch 36/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0688 - custom_layer_1_accuracy: 0.9812 - custom_layer_1_1_accuracy: 0.9817Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  60.22918773171554  Class Avg =  60.258551327483445\n",
      "276/276 [==============================] - 101s 366ms/step - loss: 0.0688 - custom_layer_1_accuracy: 0.9812 - custom_layer_1_1_accuracy: 0.9817\n",
      "\n",
      "Epoch 00037: LearningRateScheduler reducing learning rate to 9.83137096911143e-05.\n",
      "Epoch 37/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0673 - custom_layer_1_accuracy: 0.9803 - custom_layer_1_1_accuracy: 0.9850Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  60.22918773171554  Class Avg =  60.26679138717564\n",
      "276/276 [==============================] - 101s 366ms/step - loss: 0.0673 - custom_layer_1_accuracy: 0.9803 - custom_layer_1_1_accuracy: 0.9850\n",
      "\n",
      "Epoch 00038: LearningRateScheduler reducing learning rate to 9.899068514744499e-05.\n",
      "Epoch 38/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0711 - custom_layer_1_accuracy: 0.9791 - custom_layer_1_1_accuracy: 0.9807Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  60.195483653522075  Class Avg =  60.22679565112126\n",
      "276/276 [==============================] - 101s 366ms/step - loss: 0.0711 - custom_layer_1_accuracy: 0.9791 - custom_layer_1_1_accuracy: 0.9807\n",
      "\n",
      "Epoch 00039: LearningRateScheduler reducing learning rate to 9.859933679742154e-05.\n",
      "Epoch 39/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0724 - custom_layer_1_accuracy: 0.9794 - custom_layer_1_1_accuracy: 0.9811Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  60.06066734074823  Class Avg =  60.10735635895218\n",
      "276/276 [==============================] - 101s 366ms/step - loss: 0.0724 - custom_layer_1_accuracy: 0.9794 - custom_layer_1_1_accuracy: 0.9811\n",
      "\n",
      "Epoch 00040: LearningRateScheduler reducing learning rate to 9.989839286036393e-05.\n",
      "Epoch 40/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0691 - custom_layer_1_accuracy: 0.9797 - custom_layer_1_1_accuracy: 0.9817Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  60.29659588810246  Class Avg =  60.34295384318915\n",
      "276/276 [==============================] - 101s 366ms/step - loss: 0.0691 - custom_layer_1_accuracy: 0.9797 - custom_layer_1_1_accuracy: 0.9817\n",
      "\n",
      "Epoch 00041: LearningRateScheduler reducing learning rate to 1.0009231793140194e-05.\n",
      "Epoch 41/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0668 - custom_layer_1_accuracy: 0.9810 - custom_layer_1_1_accuracy: 0.9831Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  60.29659588810246  Class Avg =  60.34679138715964\n",
      "276/276 [==============================] - 101s 366ms/step - loss: 0.0668 - custom_layer_1_accuracy: 0.9810 - custom_layer_1_1_accuracy: 0.9831\n",
      "\n",
      "Epoch 00042: LearningRateScheduler reducing learning rate to 9.652347156138526e-06.\n",
      "Epoch 42/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0700 - custom_layer_1_accuracy: 0.9807 - custom_layer_1_1_accuracy: 0.9823Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  60.39770812268284  Class Avg =  60.440689692229974\n",
      "276/276 [==============================] - 101s 366ms/step - loss: 0.0700 - custom_layer_1_accuracy: 0.9807 - custom_layer_1_1_accuracy: 0.9823\n",
      "\n",
      "Epoch 00043: LearningRateScheduler reducing learning rate to 9.900137642625301e-06.\n",
      "Epoch 43/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0705 - custom_layer_1_accuracy: 0.9803 - custom_layer_1_1_accuracy: 0.9804Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  60.364004044489384  Class Avg =  60.41515296906887\n",
      "276/276 [==============================] - 101s 366ms/step - loss: 0.0705 - custom_layer_1_accuracy: 0.9803 - custom_layer_1_1_accuracy: 0.9804\n",
      "\n",
      "Epoch 00044: LearningRateScheduler reducing learning rate to 9.953352313447497e-06.\n",
      "Epoch 44/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0677 - custom_layer_1_accuracy: 0.9816 - custom_layer_1_1_accuracy: 0.9822Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  60.364004044489384  Class Avg =  60.414023025566415\n",
      "276/276 [==============================] - 101s 366ms/step - loss: 0.0677 - custom_layer_1_accuracy: 0.9816 - custom_layer_1_1_accuracy: 0.9822\n",
      "\n",
      "Epoch 00045: LearningRateScheduler reducing learning rate to 9.727545968541763e-06.\n",
      "Epoch 45/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0691 - custom_layer_1_accuracy: 0.9804 - custom_layer_1_1_accuracy: 0.9810Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  60.29659588810246  Class Avg =  60.340124720495425\n",
      "276/276 [==============================] - 101s 366ms/step - loss: 0.0691 - custom_layer_1_accuracy: 0.9804 - custom_layer_1_1_accuracy: 0.9810\n",
      "\n",
      "Epoch 00046: LearningRateScheduler reducing learning rate to 1.0448648888783333e-05.\n",
      "Epoch 46/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0686 - custom_layer_1_accuracy: 0.9802 - custom_layer_1_1_accuracy: 0.9824Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  60.364004044489384  Class Avg =  60.407921330653416\n",
      "276/276 [==============================] - 101s 366ms/step - loss: 0.0686 - custom_layer_1_accuracy: 0.9802 - custom_layer_1_1_accuracy: 0.9824\n",
      "\n",
      "Epoch 00047: LearningRateScheduler reducing learning rate to 1.0085449477456047e-05.\n",
      "Epoch 47/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0669 - custom_layer_1_accuracy: 0.9821 - custom_layer_1_1_accuracy: 0.9829Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  60.262891809909  Class Avg =  60.31402302558308\n",
      "276/276 [==============================] - 101s 366ms/step - loss: 0.0669 - custom_layer_1_accuracy: 0.9821 - custom_layer_1_1_accuracy: 0.9829\n",
      "\n",
      "Epoch 00048: LearningRateScheduler reducing learning rate to 1.0119495921113168e-05.\n",
      "Epoch 48/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0686 - custom_layer_1_accuracy: 0.9804 - custom_layer_1_1_accuracy: 0.9823Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  60.66734074823054  Class Avg =  60.7134580537652\n",
      "276/276 [==============================] - 101s 366ms/step - loss: 0.0686 - custom_layer_1_accuracy: 0.9804 - custom_layer_1_1_accuracy: 0.9823\n",
      "\n",
      "Epoch 00049: LearningRateScheduler reducing learning rate to 1.0215198162930663e-05.\n",
      "Epoch 49/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0687 - custom_layer_1_accuracy: 0.9798 - custom_layer_1_1_accuracy: 0.9811Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  60.701044826423995  Class Avg =  60.74735635884419\n",
      "276/276 [==============================] - 101s 366ms/step - loss: 0.0687 - custom_layer_1_accuracy: 0.9798 - custom_layer_1_1_accuracy: 0.9811\n",
      "\n",
      "Epoch 00050: LearningRateScheduler reducing learning rate to 1.0085437238933969e-05.\n",
      "Epoch 50/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0664 - custom_layer_1_accuracy: 0.9810 - custom_layer_1_1_accuracy: 0.9841Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  60.49882035726323  Class Avg =  60.54792133062874\n",
      "276/276 [==============================] - 101s 366ms/step - loss: 0.0664 - custom_layer_1_accuracy: 0.9810 - custom_layer_1_1_accuracy: 0.9841\n",
      "\n",
      "Epoch 00051: LearningRateScheduler reducing learning rate to 9.666814880340497e-06.\n",
      "Epoch 51/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0668 - custom_layer_1_accuracy: 0.9812 - custom_layer_1_1_accuracy: 0.9823Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  60.53252443545669  Class Avg =  60.57402302554108\n",
      "276/276 [==============================] - 101s 365ms/step - loss: 0.0668 - custom_layer_1_accuracy: 0.9812 - custom_layer_1_1_accuracy: 0.9823\n",
      "\n",
      "Epoch 00052: LearningRateScheduler reducing learning rate to 1.0132144990241171e-05.\n",
      "Epoch 52/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0632 - custom_layer_1_accuracy: 0.9828 - custom_layer_1_1_accuracy: 0.9855Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  60.364004044489384  Class Avg =  60.40622641539975\n",
      "276/276 [==============================] - 101s 365ms/step - loss: 0.0632 - custom_layer_1_accuracy: 0.9828 - custom_layer_1_1_accuracy: 0.9855\n",
      "\n",
      "Epoch 00053: LearningRateScheduler reducing learning rate to 1.0293772583597994e-05.\n",
      "Epoch 53/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0697 - custom_layer_1_accuracy: 0.9802 - custom_layer_1_1_accuracy: 0.9838Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  60.330299966295925  Class Avg =  60.3734580538232\n",
      "276/276 [==============================] - 101s 367ms/step - loss: 0.0697 - custom_layer_1_accuracy: 0.9802 - custom_layer_1_1_accuracy: 0.9838\n",
      "\n",
      "Epoch 00054: LearningRateScheduler reducing learning rate to 1.0217704130461007e-05.\n",
      "Epoch 54/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0680 - custom_layer_1_accuracy: 0.9805 - custom_layer_1_1_accuracy: 0.9813Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  60.46511627906977  Class Avg =  60.50849056632559\n",
      "276/276 [==============================] - 101s 365ms/step - loss: 0.0680 - custom_layer_1_accuracy: 0.9805 - custom_layer_1_1_accuracy: 0.9813\n",
      "\n",
      "Epoch 00055: LearningRateScheduler reducing learning rate to 9.811017946643322e-06.\n",
      "Epoch 55/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0671 - custom_layer_1_accuracy: 0.9815 - custom_layer_1_1_accuracy: 0.9829Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  60.364004044489384  Class Avg =  60.40679138715097\n",
      "276/276 [==============================] - 101s 365ms/step - loss: 0.0671 - custom_layer_1_accuracy: 0.9815 - custom_layer_1_1_accuracy: 0.9829\n",
      "\n",
      "Epoch 00056: LearningRateScheduler reducing learning rate to 1.0065155242864622e-05.\n",
      "Epoch 56/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0684 - custom_layer_1_accuracy: 0.9821 - custom_layer_1_1_accuracy: 0.9806Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  60.39770812268284  Class Avg =  60.442388871421265\n",
      "276/276 [==============================] - 101s 365ms/step - loss: 0.0684 - custom_layer_1_accuracy: 0.9821 - custom_layer_1_1_accuracy: 0.9806\n",
      "\n",
      "Epoch 00057: LearningRateScheduler reducing learning rate to 9.777653030163446e-06.\n",
      "Epoch 57/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0664 - custom_layer_1_accuracy: 0.9805 - custom_layer_1_1_accuracy: 0.9841Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  60.364004044489384  Class Avg =  60.40905553809348\n",
      "276/276 [==============================] - 101s 366ms/step - loss: 0.0664 - custom_layer_1_accuracy: 0.9805 - custom_layer_1_1_accuracy: 0.9841\n",
      "\n",
      "Epoch 00058: LearningRateScheduler reducing learning rate to 1.0011583977974515e-05.\n",
      "Epoch 58/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0666 - custom_layer_1_accuracy: 0.9812 - custom_layer_1_1_accuracy: 0.9828Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  60.364004044489384  Class Avg =  60.412893082063974\n",
      "276/276 [==============================] - 101s 365ms/step - loss: 0.0666 - custom_layer_1_accuracy: 0.9812 - custom_layer_1_1_accuracy: 0.9828\n",
      "\n",
      "Epoch 00059: LearningRateScheduler reducing learning rate to 1.011267233623473e-05.\n",
      "Epoch 59/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0665 - custom_layer_1_accuracy: 0.9846 - custom_layer_1_1_accuracy: 0.9827Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  60.66734074823054  Class Avg =  60.701254663939196\n",
      "276/276 [==============================] - 101s 366ms/step - loss: 0.0665 - custom_layer_1_accuracy: 0.9846 - custom_layer_1_1_accuracy: 0.9827\n",
      "\n",
      "Epoch 00060: LearningRateScheduler reducing learning rate to 1.0323175231841803e-05.\n",
      "Epoch 60/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0687 - custom_layer_1_accuracy: 0.9785 - custom_layer_1_1_accuracy: 0.9820Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  60.364004044489384  Class Avg =  60.414023025566415\n",
      "276/276 [==============================] - 101s 366ms/step - loss: 0.0687 - custom_layer_1_accuracy: 0.9785 - custom_layer_1_1_accuracy: 0.9820\n",
      "\n",
      "Epoch 00061: LearningRateScheduler reducing learning rate to 9.77389195374424e-07.\n",
      "Epoch 61/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0654 - custom_layer_1_accuracy: 0.9802 - custom_layer_1_1_accuracy: 0.9842Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  60.161779575328616  Class Avg =  60.21232811034608\n",
      "276/276 [==============================] - 101s 365ms/step - loss: 0.0654 - custom_layer_1_accuracy: 0.9802 - custom_layer_1_1_accuracy: 0.9842\n",
      "\n",
      "Epoch 00062: LearningRateScheduler reducing learning rate to 1.0147640166020843e-06.\n",
      "Epoch 62/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0667 - custom_layer_1_accuracy: 0.9822 - custom_layer_1_1_accuracy: 0.9832Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  60.43141220087631  Class Avg =  60.4789947769683\n",
      "276/276 [==============================] - 101s 367ms/step - loss: 0.0667 - custom_layer_1_accuracy: 0.9822 - custom_layer_1_1_accuracy: 0.9832\n",
      "\n",
      "Epoch 00063: LearningRateScheduler reducing learning rate to 1.0249185745612213e-06.\n",
      "Epoch 63/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0670 - custom_layer_1_accuracy: 0.9812 - custom_layer_1_1_accuracy: 0.9823Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  60.195483653522075  Class Avg =  60.240003655136846\n",
      "276/276 [==============================] - 101s 365ms/step - loss: 0.0670 - custom_layer_1_accuracy: 0.9812 - custom_layer_1_1_accuracy: 0.9823\n",
      "\n",
      "Epoch 00064: LearningRateScheduler reducing learning rate to 9.72634339736039e-07.\n",
      "Epoch 64/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0665 - custom_layer_1_accuracy: 0.9823 - custom_layer_1_1_accuracy: 0.9814Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  60.59993259184361  Class Avg =  60.64792133061208\n",
      "276/276 [==============================] - 101s 365ms/step - loss: 0.0665 - custom_layer_1_accuracy: 0.9823 - custom_layer_1_1_accuracy: 0.9814\n",
      "\n",
      "Epoch 00065: LearningRateScheduler reducing learning rate to 9.93618456891066e-07.\n",
      "Epoch 65/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0654 - custom_layer_1_accuracy: 0.9825 - custom_layer_1_1_accuracy: 0.9829Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  60.195483653522075  Class Avg =  60.24792133067874\n",
      "276/276 [==============================] - 101s 365ms/step - loss: 0.0654 - custom_layer_1_accuracy: 0.9825 - custom_layer_1_1_accuracy: 0.9829\n",
      "\n",
      "Epoch 00066: LearningRateScheduler reducing learning rate to 1.0305799384019324e-06.\n",
      "Epoch 66/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0678 - custom_layer_1_accuracy: 0.9794 - custom_layer_1_1_accuracy: 0.9828Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  60.262891809909  Class Avg =  60.314587997334314\n",
      "276/276 [==============================] - 101s 365ms/step - loss: 0.0678 - custom_layer_1_accuracy: 0.9794 - custom_layer_1_1_accuracy: 0.9828\n",
      "\n",
      "Epoch 00067: LearningRateScheduler reducing learning rate to 9.820816694543355e-07.\n",
      "Epoch 67/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0664 - custom_layer_1_accuracy: 0.9802 - custom_layer_1_1_accuracy: 0.9831Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  60.53252443545669  Class Avg =  60.5806896922053\n",
      "276/276 [==============================] - 101s 366ms/step - loss: 0.0664 - custom_layer_1_accuracy: 0.9802 - custom_layer_1_1_accuracy: 0.9831\n",
      "\n",
      "Epoch 00068: LearningRateScheduler reducing learning rate to 9.976137352334719e-07.\n",
      "Epoch 68/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0668 - custom_layer_1_accuracy: 0.9798 - custom_layer_1_1_accuracy: 0.9828Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  60.330299966295925  Class Avg =  60.38068969223863\n",
      "276/276 [==============================] - 101s 365ms/step - loss: 0.0668 - custom_layer_1_accuracy: 0.9798 - custom_layer_1_1_accuracy: 0.9828\n",
      "\n",
      "Epoch 00069: LearningRateScheduler reducing learning rate to 1.027329579022825e-06.\n",
      "Epoch 69/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0656 - custom_layer_1_accuracy: 0.9823 - custom_layer_1_1_accuracy: 0.9828Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  60.195483653522075  Class Avg =  60.24566144367385\n",
      "276/276 [==============================] - 101s 365ms/step - loss: 0.0656 - custom_layer_1_accuracy: 0.9823 - custom_layer_1_1_accuracy: 0.9828\n",
      "\n",
      "Epoch 00070: LearningRateScheduler reducing learning rate to 1.001270837773759e-06.\n",
      "Epoch 70/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0632 - custom_layer_1_accuracy: 0.9834 - custom_layer_1_1_accuracy: 0.9847Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  60.39770812268284  Class Avg =  60.446791387142966\n",
      "276/276 [==============================] - 101s 365ms/step - loss: 0.0632 - custom_layer_1_accuracy: 0.9834 - custom_layer_1_1_accuracy: 0.9847\n",
      "\n",
      "Epoch 00071: LearningRateScheduler reducing learning rate to 1.0022432072087352e-06.\n",
      "Epoch 71/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0691 - custom_layer_1_accuracy: 0.9795 - custom_layer_1_1_accuracy: 0.9820Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  60.39770812268284  Class Avg =  60.4473563588942\n",
      "276/276 [==============================] - 101s 365ms/step - loss: 0.0691 - custom_layer_1_accuracy: 0.9795 - custom_layer_1_1_accuracy: 0.9820\n",
      "\n",
      "Epoch 00072: LearningRateScheduler reducing learning rate to 9.895248131650786e-07.\n",
      "Epoch 72/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0686 - custom_layer_1_accuracy: 0.9837 - custom_layer_1_1_accuracy: 0.9802Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  60.43141220087631  Class Avg =  60.47571794081142\n",
      "276/276 [==============================] - 101s 366ms/step - loss: 0.0686 - custom_layer_1_accuracy: 0.9837 - custom_layer_1_1_accuracy: 0.9802\n",
      "\n",
      "Epoch 00073: LearningRateScheduler reducing learning rate to 1.0008563434717617e-06.\n",
      "Epoch 73/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0649 - custom_layer_1_accuracy: 0.9823 - custom_layer_1_1_accuracy: 0.9824Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  60.39770812268284  Class Avg =  60.44679138714297\n",
      "276/276 [==============================] - 101s 366ms/step - loss: 0.0649 - custom_layer_1_accuracy: 0.9823 - custom_layer_1_1_accuracy: 0.9824\n",
      "\n",
      "Epoch 00074: LearningRateScheduler reducing learning rate to 9.92662784005373e-07.\n",
      "Epoch 74/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0644 - custom_layer_1_accuracy: 0.9838 - custom_layer_1_1_accuracy: 0.9831Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  60.43141220087631  Class Avg =  60.4728930820553\n",
      "276/276 [==============================] - 101s 366ms/step - loss: 0.0644 - custom_layer_1_accuracy: 0.9838 - custom_layer_1_1_accuracy: 0.9831\n",
      "\n",
      "Epoch 00075: LearningRateScheduler reducing learning rate to 9.87024380157691e-07.\n",
      "Epoch 75/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0709 - custom_layer_1_accuracy: 0.9804 - custom_layer_1_1_accuracy: 0.9799Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  60.43141220087631  Class Avg =  60.48125466397319\n",
      "276/276 [==============================] - 101s 365ms/step - loss: 0.0709 - custom_layer_1_accuracy: 0.9804 - custom_layer_1_1_accuracy: 0.9799\n",
      "\n",
      "Epoch 00076: LearningRateScheduler reducing learning rate to 9.924069492331267e-07.\n",
      "Epoch 76/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0704 - custom_layer_1_accuracy: 0.9776 - custom_layer_1_1_accuracy: 0.9827Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  60.46511627906977  Class Avg =  60.51345805379852\n",
      "276/276 [==============================] - 101s 366ms/step - loss: 0.0704 - custom_layer_1_accuracy: 0.9776 - custom_layer_1_1_accuracy: 0.9827\n",
      "\n",
      "Epoch 00077: LearningRateScheduler reducing learning rate to 9.701532578627283e-07.\n",
      "Epoch 77/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0674 - custom_layer_1_accuracy: 0.9789 - custom_layer_1_1_accuracy: 0.9830Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  60.330299966295925  Class Avg =  60.37955974873619\n",
      "276/276 [==============================] - 101s 366ms/step - loss: 0.0674 - custom_layer_1_accuracy: 0.9789 - custom_layer_1_1_accuracy: 0.9830\n",
      "\n",
      "Epoch 00078: LearningRateScheduler reducing learning rate to 1.0063953347032087e-06.\n",
      "Epoch 78/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0670 - custom_layer_1_accuracy: 0.9816 - custom_layer_1_1_accuracy: 0.9837Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  60.39770812268284  Class Avg =  60.4473563588942\n",
      "276/276 [==============================] - 101s 366ms/step - loss: 0.0670 - custom_layer_1_accuracy: 0.9816 - custom_layer_1_1_accuracy: 0.9837\n",
      "\n",
      "Epoch 00079: LearningRateScheduler reducing learning rate to 9.653410589974037e-07.\n",
      "Epoch 79/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0647 - custom_layer_1_accuracy: 0.9816 - custom_layer_1_1_accuracy: 0.9830Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  60.56622851365015  Class Avg =  60.61176313852818\n",
      "276/276 [==============================] - 101s 365ms/step - loss: 0.0647 - custom_layer_1_accuracy: 0.9816 - custom_layer_1_1_accuracy: 0.9830\n",
      "\n",
      "Epoch 00080: LearningRateScheduler reducing learning rate to 9.978151091486817e-07.\n",
      "Epoch 80/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0632 - custom_layer_1_accuracy: 0.9821 - custom_layer_1_1_accuracy: 0.9845Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  60.39770812268284  Class Avg =  60.446791387142966\n",
      "276/276 [==============================] - 101s 366ms/step - loss: 0.0632 - custom_layer_1_accuracy: 0.9821 - custom_layer_1_1_accuracy: 0.9845\n",
      "\n",
      "Epoch 00081: LearningRateScheduler reducing learning rate to 1.0302447124164277e-07.\n",
      "Epoch 81/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0692 - custom_layer_1_accuracy: 0.9798 - custom_layer_1_1_accuracy: 0.9829Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  60.22918773171554  Class Avg =  60.2806896922553\n",
      "276/276 [==============================] - 101s 366ms/step - loss: 0.0692 - custom_layer_1_accuracy: 0.9798 - custom_layer_1_1_accuracy: 0.9829\n",
      "\n",
      "Epoch 00082: LearningRateScheduler reducing learning rate to 9.850247266230767e-08.\n",
      "Epoch 82/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0682 - custom_layer_1_accuracy: 0.9815 - custom_layer_1_1_accuracy: 0.9813Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  60.43141220087631  Class Avg =  60.48012472047075\n",
      "276/276 [==============================] - 101s 366ms/step - loss: 0.0682 - custom_layer_1_accuracy: 0.9815 - custom_layer_1_1_accuracy: 0.9813\n",
      "\n",
      "Epoch 00083: LearningRateScheduler reducing learning rate to 1.0114994867720772e-07.\n",
      "Epoch 83/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0695 - custom_layer_1_accuracy: 0.9794 - custom_layer_1_1_accuracy: 0.9816Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  60.29659588810246  Class Avg =  60.347921330662075\n",
      "276/276 [==============================] - 101s 366ms/step - loss: 0.0695 - custom_layer_1_accuracy: 0.9794 - custom_layer_1_1_accuracy: 0.9816\n",
      "\n",
      "Epoch 00084: LearningRateScheduler reducing learning rate to 1.0106436121431823e-07.\n",
      "Epoch 84/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0628 - custom_layer_1_accuracy: 0.9855 - custom_layer_1_1_accuracy: 0.9839Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  60.46511627906977  Class Avg =  60.51402302554975\n",
      "276/276 [==============================] - 101s 366ms/step - loss: 0.0628 - custom_layer_1_accuracy: 0.9855 - custom_layer_1_1_accuracy: 0.9839\n",
      "\n",
      "Epoch 00085: LearningRateScheduler reducing learning rate to 9.855260240274977e-08.\n",
      "Epoch 85/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0660 - custom_layer_1_accuracy: 0.9841 - custom_layer_1_1_accuracy: 0.9825Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  60.22918773171554  Class Avg =  60.28068969225531\n",
      "276/276 [==============================] - 101s 366ms/step - loss: 0.0660 - custom_layer_1_accuracy: 0.9841 - custom_layer_1_1_accuracy: 0.9825\n",
      "\n",
      "Epoch 00086: LearningRateScheduler reducing learning rate to 9.973391129412812e-08.\n",
      "Epoch 86/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0694 - custom_layer_1_accuracy: 0.9794 - custom_layer_1_1_accuracy: 0.9816Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  60.330299966295925  Class Avg =  60.3734580538232\n",
      "276/276 [==============================] - 101s 366ms/step - loss: 0.0694 - custom_layer_1_accuracy: 0.9794 - custom_layer_1_1_accuracy: 0.9816\n",
      "\n",
      "Epoch 00087: LearningRateScheduler reducing learning rate to 9.70215167898967e-08.\n",
      "Epoch 87/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0666 - custom_layer_1_accuracy: 0.9819 - custom_layer_1_1_accuracy: 0.9838Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  60.195483653522075  Class Avg =  60.24182389970338\n",
      "276/276 [==============================] - 101s 366ms/step - loss: 0.0666 - custom_layer_1_accuracy: 0.9819 - custom_layer_1_1_accuracy: 0.9838\n",
      "\n",
      "Epoch 00088: LearningRateScheduler reducing learning rate to 9.990907215602345e-08.\n",
      "Epoch 88/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0686 - custom_layer_1_accuracy: 0.9808 - custom_layer_1_1_accuracy: 0.9832Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  60.46511627906977  Class Avg =  60.51458799730097\n",
      "276/276 [==============================] - 101s 366ms/step - loss: 0.0686 - custom_layer_1_accuracy: 0.9808 - custom_layer_1_1_accuracy: 0.9832\n",
      "\n",
      "Epoch 00089: LearningRateScheduler reducing learning rate to 9.867772377204092e-08.\n",
      "Epoch 89/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0674 - custom_layer_1_accuracy: 0.9817 - custom_layer_1_1_accuracy: 0.9823Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  60.46511627906977  Class Avg =  60.51458799730097\n",
      "276/276 [==============================] - 101s 366ms/step - loss: 0.0674 - custom_layer_1_accuracy: 0.9817 - custom_layer_1_1_accuracy: 0.9823\n",
      "\n",
      "Epoch 00090: LearningRateScheduler reducing learning rate to 1.0184086476294865e-07.\n",
      "Epoch 90/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0679 - custom_layer_1_accuracy: 0.9827 - custom_layer_1_1_accuracy: 0.9802Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  60.29659588810246  Class Avg =  60.348486302413306\n",
      "276/276 [==============================] - 101s 367ms/step - loss: 0.0679 - custom_layer_1_accuracy: 0.9827 - custom_layer_1_1_accuracy: 0.9802\n",
      "\n",
      "Epoch 00091: LearningRateScheduler reducing learning rate to 1.005728701602355e-07.\n",
      "Epoch 91/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0657 - custom_layer_1_accuracy: 0.9823 - custom_layer_1_1_accuracy: 0.9834Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  60.364004044489384  Class Avg =  60.40849056634226\n",
      "276/276 [==============================] - 101s 367ms/step - loss: 0.0657 - custom_layer_1_accuracy: 0.9823 - custom_layer_1_1_accuracy: 0.9834\n",
      "\n",
      "Epoch 00092: LearningRateScheduler reducing learning rate to 9.965593825498392e-08.\n",
      "Epoch 92/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0651 - custom_layer_1_accuracy: 0.9822 - custom_layer_1_1_accuracy: 0.9829Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  60.29659588810246  Class Avg =  60.34497540653073\n",
      "276/276 [==============================] - 101s 366ms/step - loss: 0.0651 - custom_layer_1_accuracy: 0.9822 - custom_layer_1_1_accuracy: 0.9829\n",
      "\n",
      "Epoch 00093: LearningRateScheduler reducing learning rate to 9.967349223587221e-08.\n",
      "Epoch 93/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0676 - custom_layer_1_accuracy: 0.9814 - custom_layer_1_1_accuracy: 0.9839Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  60.195483653522075  Class Avg =  60.2406896922633\n",
      "276/276 [==============================] - 101s 366ms/step - loss: 0.0676 - custom_layer_1_accuracy: 0.9814 - custom_layer_1_1_accuracy: 0.9839\n",
      "\n",
      "Epoch 00094: LearningRateScheduler reducing learning rate to 9.763406922927804e-08.\n",
      "Epoch 94/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0676 - custom_layer_1_accuracy: 0.9795 - custom_layer_1_1_accuracy: 0.9833Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  60.262891809909  Class Avg =  60.31345805383185\n",
      "276/276 [==============================] - 101s 366ms/step - loss: 0.0676 - custom_layer_1_accuracy: 0.9795 - custom_layer_1_1_accuracy: 0.9833\n",
      "\n",
      "Epoch 00095: LearningRateScheduler reducing learning rate to 9.732732759911636e-08.\n",
      "Epoch 95/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0652 - custom_layer_1_accuracy: 0.9808 - custom_layer_1_1_accuracy: 0.9844Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  60.66734074823054  Class Avg =  60.70012472043674\n",
      "276/276 [==============================] - 101s 366ms/step - loss: 0.0652 - custom_layer_1_accuracy: 0.9808 - custom_layer_1_1_accuracy: 0.9844\n",
      "\n",
      "Epoch 00096: LearningRateScheduler reducing learning rate to 9.710775251941279e-08.\n",
      "Epoch 96/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0681 - custom_layer_1_accuracy: 0.9837 - custom_layer_1_1_accuracy: 0.9816Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  60.46511627906977  Class Avg =  60.512893082047306\n",
      "276/276 [==============================] - 101s 366ms/step - loss: 0.0681 - custom_layer_1_accuracy: 0.9837 - custom_layer_1_1_accuracy: 0.9816\n",
      "\n",
      "Epoch 00097: LearningRateScheduler reducing learning rate to 9.931697946757045e-08.\n",
      "Epoch 97/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0694 - custom_layer_1_accuracy: 0.9805 - custom_layer_1_1_accuracy: 0.9833Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  60.46511627906977  Class Avg =  60.50735635888552\n",
      "276/276 [==============================] - 101s 366ms/step - loss: 0.0694 - custom_layer_1_accuracy: 0.9805 - custom_layer_1_1_accuracy: 0.9833\n",
      "\n",
      "Epoch 00098: LearningRateScheduler reducing learning rate to 9.633879832807595e-08.\n",
      "Epoch 98/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0690 - custom_layer_1_accuracy: 0.9796 - custom_layer_1_1_accuracy: 0.9825Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  60.53252443545669  Class Avg =  60.575722204732386\n",
      "276/276 [==============================] - 101s 366ms/step - loss: 0.0690 - custom_layer_1_accuracy: 0.9796 - custom_layer_1_1_accuracy: 0.9825\n",
      "\n",
      "Epoch 00099: LearningRateScheduler reducing learning rate to 9.891962982546465e-08.\n",
      "Epoch 99/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0635 - custom_layer_1_accuracy: 0.9834 - custom_layer_1_1_accuracy: 0.9833Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  60.364004044489384  Class Avg =  60.40622641539974\n",
      "276/276 [==============================] - 101s 366ms/step - loss: 0.0635 - custom_layer_1_accuracy: 0.9834 - custom_layer_1_1_accuracy: 0.9833\n",
      "\n",
      "Epoch 00100: LearningRateScheduler reducing learning rate to 9.775351351228596e-08.\n",
      "Epoch 100/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0674 - custom_layer_1_accuracy: 0.9796 - custom_layer_1_1_accuracy: 0.9839Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  60.364004044489384  Class Avg =  60.41345805381519\n",
      "276/276 [==============================] - 101s 366ms/step - loss: 0.0674 - custom_layer_1_accuracy: 0.9796 - custom_layer_1_1_accuracy: 0.9839\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "892"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top1 = AverageMeter()\n",
    "class_avg = ClassAverageMeter(50)\n",
    "class CustomCallback(keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        top1.reset()\n",
    "        class_avg.reset(50)\n",
    "        predictions = model.predict((test_data, final_test_labels_cat, test_text_features, testing_phase))\n",
    "        print(\"Predictions Using Average result\")\n",
    "        avg_output = (predictions[0]+predictions[1])/2\n",
    "        prec1,class_acc,class_cnt,prec_prob = accuracy(avg_output, final_test_labels, 50)\n",
    "\n",
    "        top1.update(prec1, avg_output.shape[0])\n",
    "        class_avg.update(class_acc,class_cnt,prec_prob)\n",
    "\n",
    "        print(\"Epocs = \", i, \" Top1 = \", top1.avg, \" Class Avg = \", class_avg.avg)\n",
    "        del predictions\n",
    "        gc.collect()\n",
    "\n",
    "drop = 20\n",
    "def sgdr(epoch):\n",
    "    lr = learning_rate*(0.5 ** (epoch // drop))\n",
    "    return lr\n",
    "\n",
    "reduce_lr = tf.keras.callbacks.LearningRateScheduler(sgdr, verbose=True)\n",
    "on_epoch_callback = CustomCallback()\n",
    "k_list = [8,10,12]\n",
    "alpha_list = [0.7,0.8,1]\n",
    "cmps_list = [20, 50, 100]\n",
    "import gc\n",
    "\n",
    "\n",
    "training_phase = np.ones(shape=len(train_data), dtype=np.int64)\n",
    "final_train_labels_shuffled_cat = tf.keras.utils.to_categorical(final_train_labels_shuffled, num_classes=150)\n",
    "final_test_labels_cat = tf.keras.utils.to_categorical(final_test_labels, num_classes=50)\n",
    "\n",
    "model.fit((train_data_shuffled, final_train_labels_shuffled_cat, train_text_features_shuffled, training_phase),\n",
    "            final_train_labels_shuffled_cat, epochs=100, batch_size=32,\n",
    "            callbacks=[reduce_lr, on_epoch_callback])\n",
    "\n",
    "\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Original AREN in Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLayer_ORIG(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(CustomLayer, self).__init__()\n",
    "        self.shared_train_init = tf.constant_initializer(train_weights_matrix_numpy.transpose())\n",
    "        self.shared_train = Dense(len(train_weights_matrix_numpy), use_bias=False, kernel_initializer=self.shared_train_init, trainable=False, name=\"shared_train\")\n",
    "        self.shared_train.trainable = False\n",
    "        self.shared_test_init = tf.constant_initializer(test_weights_matrix_numpy.transpose())\n",
    "        self.shared_test = Dense(len(test_weights_matrix_numpy), use_bias=False, kernel_initializer=self.shared_test_init, trainable=False, name=\"shared_test\")\n",
    "        self.shared_test.trainable = False\n",
    "        \n",
    "    def train_step(self, are_features, acse_features):\n",
    "        return self.shared_train(are_features), self.shared_train(acse_features)\n",
    "    \n",
    "    def test_step(self, are_features, acse_features):\n",
    "        return self.shared_test(are_features), self.shared_test(acse_features)\n",
    "\n",
    "\n",
    "    def call(self, are_features, acse_features, is_training):\n",
    "        return tf.cond(tf.equal(is_training, 1),\n",
    "                       lambda: self.train_step(are_features, acse_features),\n",
    "                       lambda: self.test_step(are_features, acse_features))\n",
    "    \n",
    "class CustomPreprocessing_ORIG(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(CustomPreprocessing, self).__init__()\n",
    "        self.random_crop = tf.keras.layers.experimental.preprocessing.RandomCrop(224, 224)\n",
    "        self.random_flip = tf.keras.layers.experimental.preprocessing.RandomFlip('horizontal')\n",
    "        self.center_crop = tf.keras.layers.experimental.preprocessing.CenterCrop(224, 224)\n",
    "        self.mean_tensor = tf.constant([0.406, 0.456, 0.485], name=\"mean_tensor\")\n",
    "        self.std_tensor = tf.constant([0.225, 0.224, 0.229], name=\"std_tensor\")\n",
    "        self.preproc_layer = Lambda(lambda x: (tf.cast(x, tf.float32) - self.mean_tensor) / self.std_tensor)\n",
    "        \n",
    "    def train_preprocess(self, image_features):\n",
    "        image_features = self.random_crop(image_features)\n",
    "        image_features = self.random_flip(image_features)\n",
    "        image_features = self.preproc_layer(image_features)\n",
    "        return image_features\n",
    "    \n",
    "    def test_preprocess(self, image_features):\n",
    "        image_features = self.center_crop(image_features)\n",
    "        image_features = self.preproc_layer(image_features)\n",
    "        return image_features\n",
    "\n",
    "    def call(self, image_features, is_training):\n",
    "        return tf.cond(tf.equal(is_training, 1),\n",
    "                       lambda: self.train_preprocess(image_features),\n",
    "                       lambda: self.test_preprocess(image_features))\n",
    "\n",
    "class MyModel_ORIG(tf.keras.Model):\n",
    "    def __init__(self, k, alpha, cmps):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.resnet_base = ResNet101(include_top=False, input_shape=(224,224,3), weights=\"imagenet\")\n",
    "        self.resnet = Model(inputs=self.resnet_base.inputs, outputs=self.resnet_base.get_layer(\"conv5_block3_3_bn\").output)\n",
    "        self.flatten = Flatten()\n",
    "        self.custom_preprocess = CustomPreprocessing_ORIG()\n",
    "        self.conv_channels = 2048\n",
    "        self.threshold = alpha\n",
    "        self.parts = k\n",
    "        self.map_size = 7\n",
    "        self.pool2d = MaxPool2D(pool_size=self.map_size, strides=self.map_size)\n",
    "        self.conv2d = Conv2D(self.parts, 1, kernel_initializer=tf.keras.initializers.orthogonal)\n",
    "        self.resnet_features = None\n",
    "        self.resnet_feature_shapes = None\n",
    "        self.conv_bilinear = Conv2D(cmps, 1, kernel_initializer=tf.keras.initializers.orthogonal)\n",
    "        self.coef = self.map_size * self.map_size\n",
    "        self.p_linear = Dense(1024, use_bias=False, kernel_initializer=tf.keras.initializers.orthogonal)\n",
    "        self.b_linear = Dense(1024, use_bias=False, kernel_initializer=tf.keras.initializers.orthogonal)\n",
    "        self.attr_linear = Dense(1024, use_bias=False, kernel_initializer=tf.keras.initializers.orthogonal)\n",
    "        self.dropout_p = Dropout(0.4)\n",
    "        self.dropout_b = Dropout(0.4)\n",
    "        self.dropout_attr = Dropout(0.4)\n",
    "        self.batchnorm = BatchNormalization()\n",
    "        self.batchnorm_bilinear = BatchNormalization()\n",
    "        self.custom_layer = CustomLayer_ORIG()\n",
    "        self.shared_train_init = tf.constant_initializer(tf.keras.backend.l2_normalize(train_weights_matrix, axis=1).numpy())\n",
    "        self.dense_se_1 = Dense(512, use_bias=False, kernel_initializer=tf.keras.initializers.orthogonal)\n",
    "        self.dense_se_2 = Dense(2048, use_bias=False, kernel_initializer=tf.keras.initializers.orthogonal)\n",
    "        self.gap = GlobalAveragePooling2D()\n",
    "\n",
    "    def call(self, input1, input2, input3):\n",
    "        input1 = self.custom_preprocess(input1, input3[0])\n",
    "\n",
    "        feat = self.resnet(input1)\n",
    "        feat = tf.keras.activations.relu(feat)\n",
    "\n",
    "        self.resnet_features = feat\n",
    "        self.resnet_feature_shapes = tf.shape(feat)\n",
    "        \n",
    "        # ==== Self Attention ==== #\n",
    "        att_weight = self.conv2d(feat)\n",
    "        att_weight = tf.keras.activations.sigmoid(att_weight)\n",
    "        \n",
    "        batch = tf.shape(att_weight)[0]\n",
    "        height = tf.convert_to_tensor(att_weight.shape[1])\n",
    "        width = tf.convert_to_tensor(att_weight.shape[2])\n",
    "        channels = tf.convert_to_tensor(att_weight.shape[3])\n",
    "        \n",
    "        global_max_value = tf.math.reduce_max(tf.reshape(att_weight, [batch, -1]), axis=1)\n",
    "        local_max_value = tf.math.reduce_max(tf.reshape(att_weight, [batch, -1, self.parts]), axis=1)\n",
    "\n",
    "        global_max_value = tf.broadcast_to(tf.expand_dims(global_max_value, axis=-1), shape=[batch, channels])\n",
    "        threshold_value = self.threshold * global_max_value\n",
    "\n",
    "        mask = tf.broadcast_to(tf.expand_dims(tf.expand_dims(tf.cast(tf.math.greater_equal(local_max_value, threshold_value), dtype=tf.float32), axis=1), axis=1),\n",
    "                               shape=[batch, height, width, self.parts])\n",
    "\n",
    "        attention_weights = tf.math.multiply(att_weight, mask)\n",
    "        attention_weights = tf.transpose(attention_weights, perm=[3, 0, 1, 2])\n",
    "        attention_weights = tf.expand_dims(attention_weights, axis=2)\n",
    "        attention_weights_shape = tf.shape(attention_weights)\n",
    "        attention_weights = tf.broadcast_to(attention_weights, shape=[attention_weights_shape[0], attention_weights_shape[1], self.conv_channels, attention_weights_shape[3], attention_weights_shape[4]])\n",
    "        attention_weights = tf.transpose(attention_weights, perm=[0, 1, 3, 4, 2])\n",
    "        features = tf.broadcast_to(tf.expand_dims(self.resnet_features, axis=0), [self.parts, self.resnet_feature_shapes[0], self.resnet_feature_shapes[1], self.resnet_feature_shapes[2], self.resnet_feature_shapes[3]])\n",
    "        Y = tf.math.multiply(attention_weights, features)\n",
    "        \n",
    "        # ==== ARE Network ==== #\n",
    "        are_output = tf.map_fn(lambda vec: self.pool2d(vec), Y)\n",
    "        are_output = tf.squeeze(are_output, [2,3])\n",
    "        are_output = tf.transpose(are_output, [1,0,2])\n",
    "        weights_batch = tf.shape(are_output)[0]\n",
    "        weights_channel = are_output.shape[1]\n",
    "        weights_h = are_output.shape[2]\n",
    "        are_output = tf.reshape(are_output, (weights_batch, weights_channel*weights_h))\n",
    "\n",
    "\n",
    "        # ==== ACSE Network ==== #\n",
    "        \n",
    "        Y_shape = tf.shape(Y)\n",
    "        Y_temp = tf.reshape(Y, shape=[Y.shape[0], Y_shape[1], Y.shape[2]*Y.shape[3], Y.shape[4]])\n",
    "        self.bilinear = self.conv_bilinear(self.resnet_features)\n",
    "        batch = tf.shape(self.bilinear)[0]\n",
    "        h = self.bilinear.shape[1]\n",
    "        w = self.bilinear.shape[2]\n",
    "        channels = self.bilinear.shape[3]\n",
    "        X = tf.reshape(self.bilinear, shape=[batch, h*w, channels])\n",
    "        X = tf.transpose(X, [0,2,1])\n",
    "        acse_output = tf.map_fn(lambda vec: (tf.einsum('ikj,ijl -> ikl', X, vec) / self.coef), Y_temp)\n",
    "        acse_output_shape = tf.shape(acse_output)\n",
    "        acse_output = tf.reshape(acse_output, shape=[acse_output.shape[0], acse_output_shape[1],\n",
    "                                                     acse_output.shape[2]*acse_output.shape[3]])\n",
    "        acse_output = tf.math.reduce_max(acse_output, axis=0)\n",
    "\n",
    "        are_feat = self.dropout_p(self.p_linear(are_output))\n",
    "        acse_feat = self.dropout_b(self.b_linear(acse_output))\n",
    "        \n",
    "        are_feat, acse_feat = self.custom_layer(are_feat, acse_feat, input3[0])\n",
    "\n",
    "        return are_feat, acse_feat\n",
    "\n",
    "    def model(self):\n",
    "        inp = Input(shape=(256,256,3))\n",
    "        inp1 = Input(shape=(150))\n",
    "        inp2 = Input(shape=(312))\n",
    "        inp3 = Input(shape=())\n",
    "        return Model(inputs=(inp, inp1, inp2, inp3), outputs=self.call(inp, inp2, inp3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_obj = MyModel_ORIG(12, 1, 20)\n",
    "model = model_obj.model()\n",
    "learning_rate = 0.001\n",
    "\n",
    "cls_loss = (classification_loss(model.outputs[0], model.inputs[1]) +\n",
    "                classification_loss(model.outputs[1], model.inputs[1]))/2\n",
    "model.add_loss(cls_loss)\n",
    "\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate, momentum=0.9, decay=0.0005)\n",
    "model.compile(optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "\n",
    "for layer in model.layers:\n",
    "    if \"custom_layer\" in layer.name:\n",
    "        layer.trainable = False\n",
    "        print(layer.trainable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00001: LearningRateScheduler reducing learning rate to 0.001017305241766734.\n",
      "Epoch 1/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 3.0107 - custom_layer_accuracy: 0.2715 - custom_layer_1_accuracy: 0.2487Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  43.30974047859791  Class Avg =  43.41709335766346\n",
      "276/276 [==============================] - 105s 381ms/step - loss: 3.0107 - custom_layer_accuracy: 0.2715 - custom_layer_1_accuracy: 0.2487\n",
      "\n",
      "Epoch 00002: LearningRateScheduler reducing learning rate to 0.0009949384061755256.\n",
      "Epoch 2/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 1.4867 - custom_layer_accuracy: 0.5659 - custom_layer_1_accuracy: 0.5621Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  51.23019885406134  Class Avg =  51.247394735892115\n",
      "276/276 [==============================] - 102s 370ms/step - loss: 1.4867 - custom_layer_accuracy: 0.5659 - custom_layer_1_accuracy: 0.5621\n",
      "\n",
      "Epoch 00003: LearningRateScheduler reducing learning rate to 0.001003768470648339.\n",
      "Epoch 3/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 1.0042 - custom_layer_accuracy: 0.6928 - custom_layer_1_accuracy: 0.6969Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  54.97135153353556  Class Avg =  55.126496556678624\n",
      "276/276 [==============================] - 102s 370ms/step - loss: 1.0042 - custom_layer_accuracy: 0.6928 - custom_layer_1_accuracy: 0.6969\n",
      "\n",
      "Epoch 00004: LearningRateScheduler reducing learning rate to 0.0010039006973054229.\n",
      "Epoch 4/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.7634 - custom_layer_accuracy: 0.7594 - custom_layer_1_accuracy: 0.7699Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  59.95955510616785  Class Avg =  60.063452865721835\n",
      "276/276 [==============================] - 102s 370ms/step - loss: 0.7634 - custom_layer_accuracy: 0.7594 - custom_layer_1_accuracy: 0.7699\n",
      "\n",
      "Epoch 00005: LearningRateScheduler reducing learning rate to 0.0009864752492337156.\n",
      "Epoch 5/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.6005 - custom_layer_accuracy: 0.8146 - custom_layer_1_accuracy: 0.8081Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  58.94843276036401  Class Avg =  58.99243302337527\n",
      "276/276 [==============================] - 102s 369ms/step - loss: 0.6005 - custom_layer_accuracy: 0.8146 - custom_layer_1_accuracy: 0.8081\n",
      "\n",
      "Epoch 00006: LearningRateScheduler reducing learning rate to 0.0009921607327616627.\n",
      "Epoch 6/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.4930 - custom_layer_accuracy: 0.8353 - custom_layer_1_accuracy: 0.8440Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  56.454330974047856  Class Avg =  56.493251395278314\n",
      "276/276 [==============================] - 102s 370ms/step - loss: 0.4930 - custom_layer_accuracy: 0.8353 - custom_layer_1_accuracy: 0.8440\n",
      "\n",
      "Epoch 00007: LearningRateScheduler reducing learning rate to 0.0010216020113223592.\n",
      "Epoch 7/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.4024 - custom_layer_accuracy: 0.8686 - custom_layer_1_accuracy: 0.8727Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  56.757667677789016  Class Avg =  56.815545089161226\n",
      "276/276 [==============================] - 102s 370ms/step - loss: 0.4024 - custom_layer_accuracy: 0.8686 - custom_layer_1_accuracy: 0.8727\n",
      "\n",
      "Epoch 00008: LearningRateScheduler reducing learning rate to 0.0009838244336040584.\n",
      "Epoch 8/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.3407 - custom_layer_accuracy: 0.8886 - custom_layer_1_accuracy: 0.8928Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  58.308055274688236  Class Avg =  58.347178786197965\n",
      "276/276 [==============================] - 102s 370ms/step - loss: 0.3407 - custom_layer_accuracy: 0.8886 - custom_layer_1_accuracy: 0.8928\n",
      "\n",
      "Epoch 00009: LearningRateScheduler reducing learning rate to 0.001024179942879734.\n",
      "Epoch 9/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.2971 - custom_layer_accuracy: 0.9009 - custom_layer_1_accuracy: 0.9078Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  57.53286147623862  Class Avg =  57.57019491314993\n",
      "276/276 [==============================] - 102s 370ms/step - loss: 0.2971 - custom_layer_accuracy: 0.9009 - custom_layer_1_accuracy: 0.9078\n",
      "\n",
      "Epoch 00010: LearningRateScheduler reducing learning rate to 0.0009877781557714807.\n",
      "Epoch 10/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.2499 - custom_layer_accuracy: 0.9174 - custom_layer_1_accuracy: 0.9195Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  59.89214694978092  Class Avg =  59.9265730022046\n",
      "276/276 [==============================] - 102s 370ms/step - loss: 0.2499 - custom_layer_accuracy: 0.9174 - custom_layer_1_accuracy: 0.9195\n",
      "\n",
      "Epoch 00011: LearningRateScheduler reducing learning rate to 0.0009845633739057975.\n",
      "Epoch 11/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.2110 - custom_layer_accuracy: 0.9273 - custom_layer_1_accuracy: 0.9363Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  57.09470845972363  Class Avg =  57.163801290845\n",
      "276/276 [==============================] - 102s 371ms/step - loss: 0.2110 - custom_layer_accuracy: 0.9273 - custom_layer_1_accuracy: 0.9363\n",
      "\n",
      "Epoch 00012: LearningRateScheduler reducing learning rate to 0.0010050130018393487.\n",
      "Epoch 12/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.1927 - custom_layer_accuracy: 0.9344 - custom_layer_1_accuracy: 0.9404Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  57.836198179979775  Class Avg =  57.857369140646924\n",
      "276/276 [==============================] - 103s 372ms/step - loss: 0.1927 - custom_layer_accuracy: 0.9344 - custom_layer_1_accuracy: 0.9404\n",
      "\n",
      "Epoch 00013: LearningRateScheduler reducing learning rate to 0.0010178291806860129.\n",
      "Epoch 13/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.1645 - custom_layer_accuracy: 0.9471 - custom_layer_1_accuracy: 0.9536Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  58.13953488372093  Class Avg =  58.09899141664743\n",
      "276/276 [==============================] - 102s 371ms/step - loss: 0.1645 - custom_layer_accuracy: 0.9471 - custom_layer_1_accuracy: 0.9536\n",
      "\n",
      "Epoch 00014: LearningRateScheduler reducing learning rate to 0.001006237660022856.\n",
      "Epoch 14/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.1488 - custom_layer_accuracy: 0.9531 - custom_layer_1_accuracy: 0.9553Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  57.66767778901247  Class Avg =  57.81140663282673\n",
      "276/276 [==============================] - 102s 371ms/step - loss: 0.1488 - custom_layer_accuracy: 0.9531 - custom_layer_1_accuracy: 0.9553\n",
      "\n",
      "Epoch 00015: LearningRateScheduler reducing learning rate to 0.0009765382961587022.\n",
      "Epoch 15/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.1346 - custom_layer_accuracy: 0.9565 - custom_layer_1_accuracy: 0.9595Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  58.40916750926862  Class Avg =  58.46408286280419\n",
      "276/276 [==============================] - 103s 372ms/step - loss: 0.1346 - custom_layer_accuracy: 0.9565 - custom_layer_1_accuracy: 0.9595\n",
      "\n",
      "Epoch 00016: LearningRateScheduler reducing learning rate to 0.0009940690492020272.\n",
      "Epoch 16/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.1239 - custom_layer_accuracy: 0.9613 - custom_layer_1_accuracy: 0.9637Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  59.150657229524775  Class Avg =  59.19726802417812\n",
      "276/276 [==============================] - 103s 372ms/step - loss: 0.1239 - custom_layer_accuracy: 0.9613 - custom_layer_1_accuracy: 0.9637\n",
      "\n",
      "Epoch 00017: LearningRateScheduler reducing learning rate to 0.000989590297197891.\n",
      "Epoch 17/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.1062 - custom_layer_accuracy: 0.9663 - custom_layer_1_accuracy: 0.9728Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  58.54398382204247  Class Avg =  58.59160125101539\n",
      "276/276 [==============================] - 103s 372ms/step - loss: 0.1062 - custom_layer_accuracy: 0.9663 - custom_layer_1_accuracy: 0.9728\n",
      "\n",
      "Epoch 00018: LearningRateScheduler reducing learning rate to 0.0010464078398701468.\n",
      "Epoch 18/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0919 - custom_layer_accuracy: 0.9719 - custom_layer_1_accuracy: 0.9745Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  60.12807549713515  Class Avg =  60.160601357351204\n",
      "276/276 [==============================] - 103s 372ms/step - loss: 0.0919 - custom_layer_accuracy: 0.9719 - custom_layer_1_accuracy: 0.9745\n",
      "\n",
      "Epoch 00019: LearningRateScheduler reducing learning rate to 0.0010329459484495053.\n",
      "Epoch 19/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0906 - custom_layer_accuracy: 0.9709 - custom_layer_1_accuracy: 0.9747Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  58.94843276036401  Class Avg =  59.012574962083924\n",
      "276/276 [==============================] - 102s 371ms/step - loss: 0.0906 - custom_layer_accuracy: 0.9709 - custom_layer_1_accuracy: 0.9747\n",
      "\n",
      "Epoch 00020: LearningRateScheduler reducing learning rate to 0.000999753575693617.\n",
      "Epoch 20/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0803 - custom_layer_accuracy: 0.9761 - custom_layer_1_accuracy: 0.9783Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  59.015840916750925  Class Avg =  59.03454138830522\n",
      "276/276 [==============================] - 103s 371ms/step - loss: 0.0803 - custom_layer_accuracy: 0.9761 - custom_layer_1_accuracy: 0.9783\n",
      "\n",
      "Epoch 00021: LearningRateScheduler reducing learning rate to 0.00010256256057860767.\n",
      "Epoch 21/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0778 - custom_layer_accuracy: 0.9779 - custom_layer_1_accuracy: 0.9806Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  58.7799123693967  Class Avg =  58.78888786374718\n",
      "276/276 [==============================] - 103s 372ms/step - loss: 0.0778 - custom_layer_accuracy: 0.9779 - custom_layer_1_accuracy: 0.9806\n",
      "\n",
      "Epoch 00022: LearningRateScheduler reducing learning rate to 9.88776559023814e-05.\n",
      "Epoch 22/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0701 - custom_layer_accuracy: 0.9813 - custom_layer_1_accuracy: 0.9828Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  59.35288169868554  Class Avg =  59.41000410162182\n",
      "276/276 [==============================] - 102s 369ms/step - loss: 0.0701 - custom_layer_accuracy: 0.9813 - custom_layer_1_accuracy: 0.9828\n",
      "\n",
      "Epoch 00023: LearningRateScheduler reducing learning rate to 9.607878838752646e-05.\n",
      "Epoch 23/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0700 - custom_layer_accuracy: 0.9779 - custom_layer_1_accuracy: 0.9823Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  59.42028985507246  Class Avg =  59.464350577013754\n",
      "276/276 [==============================] - 102s 370ms/step - loss: 0.0700 - custom_layer_accuracy: 0.9779 - custom_layer_1_accuracy: 0.9823\n",
      "\n",
      "Epoch 00024: LearningRateScheduler reducing learning rate to 0.0001020026551174654.\n",
      "Epoch 24/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0690 - custom_layer_accuracy: 0.9786 - custom_layer_1_accuracy: 0.9827Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  59.28547354229862  Class Avg =  59.32358002257454\n",
      "276/276 [==============================] - 102s 371ms/step - loss: 0.0690 - custom_layer_accuracy: 0.9786 - custom_layer_1_accuracy: 0.9827\n",
      "\n",
      "Epoch 00025: LearningRateScheduler reducing learning rate to 0.00010286787163450256.\n",
      "Epoch 25/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0630 - custom_layer_accuracy: 0.9833 - custom_layer_1_accuracy: 0.9842Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  59.35288169868554  Class Avg =  59.39081166098131\n",
      "276/276 [==============================] - 102s 370ms/step - loss: 0.0630 - custom_layer_accuracy: 0.9833 - custom_layer_1_accuracy: 0.9842\n",
      "\n",
      "Epoch 00026: LearningRateScheduler reducing learning rate to 0.00010031601549611907.\n",
      "Epoch 26/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0672 - custom_layer_accuracy: 0.9795 - custom_layer_1_accuracy: 0.9838Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  59.184361307718234  Class Avg =  59.22721198384902\n",
      "276/276 [==============================] - 102s 371ms/step - loss: 0.0672 - custom_layer_accuracy: 0.9795 - custom_layer_1_accuracy: 0.9838\n",
      "\n",
      "Epoch 00027: LearningRateScheduler reducing learning rate to 0.00010121277492289661.\n",
      "Epoch 27/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0623 - custom_layer_accuracy: 0.9820 - custom_layer_1_accuracy: 0.9856Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  59.28547354229862  Class Avg =  59.32923354717393\n",
      "276/276 [==============================] - 102s 371ms/step - loss: 0.0623 - custom_layer_accuracy: 0.9820 - custom_layer_1_accuracy: 0.9856\n",
      "\n",
      "Epoch 00028: LearningRateScheduler reducing learning rate to 9.929939920724998e-05.\n",
      "Epoch 28/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0615 - custom_layer_accuracy: 0.9812 - custom_layer_1_accuracy: 0.9867Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  59.11695315133131  Class Avg =  59.15533950605723\n",
      "276/276 [==============================] - 102s 370ms/step - loss: 0.0615 - custom_layer_accuracy: 0.9812 - custom_layer_1_accuracy: 0.9867\n",
      "\n",
      "Epoch 00029: LearningRateScheduler reducing learning rate to 9.961069895716418e-05.\n",
      "Epoch 29/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0627 - custom_layer_accuracy: 0.9840 - custom_layer_1_accuracy: 0.9840Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  59.21806538591169  Class Avg =  59.26450392652252\n",
      "276/276 [==============================] - 102s 370ms/step - loss: 0.0627 - custom_layer_accuracy: 0.9840 - custom_layer_1_accuracy: 0.9840\n",
      "\n",
      "Epoch 00030: LearningRateScheduler reducing learning rate to 9.844222512282775e-05.\n",
      "Epoch 30/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0607 - custom_layer_accuracy: 0.9828 - custom_layer_1_accuracy: 0.9866Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  59.45399393326593  Class Avg =  59.50935034857887\n",
      "276/276 [==============================] - 102s 371ms/step - loss: 0.0607 - custom_layer_accuracy: 0.9828 - custom_layer_1_accuracy: 0.9866\n",
      "\n",
      "Epoch 00031: LearningRateScheduler reducing learning rate to 0.00010045818296792624.\n",
      "Epoch 31/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0583 - custom_layer_accuracy: 0.9840 - custom_layer_1_accuracy: 0.9863Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  59.35288169868554  Class Avg =  59.402118710180105\n",
      "276/276 [==============================] - 102s 370ms/step - loss: 0.0583 - custom_layer_accuracy: 0.9840 - custom_layer_1_accuracy: 0.9863\n",
      "\n",
      "Epoch 00032: LearningRateScheduler reducing learning rate to 9.83053037928071e-05.\n",
      "Epoch 32/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0571 - custom_layer_accuracy: 0.9839 - custom_layer_1_accuracy: 0.9867Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  59.31917762049208  Class Avg =  59.36652548984743\n",
      "276/276 [==============================] - 102s 370ms/step - loss: 0.0571 - custom_layer_accuracy: 0.9839 - custom_layer_1_accuracy: 0.9867\n",
      "\n",
      "Epoch 00033: LearningRateScheduler reducing learning rate to 0.00010350228056137197.\n",
      "Epoch 33/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0558 - custom_layer_accuracy: 0.9848 - custom_layer_1_accuracy: 0.9863Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  59.386585776879  Class Avg =  59.4291162888692\n",
      "276/276 [==============================] - 102s 371ms/step - loss: 0.0558 - custom_layer_accuracy: 0.9848 - custom_layer_1_accuracy: 0.9863\n",
      "\n",
      "Epoch 00034: LearningRateScheduler reducing learning rate to 0.00010231249023476589.\n",
      "Epoch 34/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0567 - custom_layer_accuracy: 0.9830 - custom_layer_1_accuracy: 0.9866Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  59.21806538591169  Class Avg =  59.263579565732755\n",
      "276/276 [==============================] - 102s 370ms/step - loss: 0.0567 - custom_layer_accuracy: 0.9830 - custom_layer_1_accuracy: 0.9866\n",
      "\n",
      "Epoch 00035: LearningRateScheduler reducing learning rate to 9.973542738910682e-05.\n",
      "Epoch 35/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0580 - custom_layer_accuracy: 0.9828 - custom_layer_1_accuracy: 0.9879Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  59.25176946410516  Class Avg =  59.29747787081175\n",
      "276/276 [==============================] - 102s 369ms/step - loss: 0.0580 - custom_layer_accuracy: 0.9828 - custom_layer_1_accuracy: 0.9879\n",
      "\n",
      "Epoch 00036: LearningRateScheduler reducing learning rate to 0.00010417252314962198.\n",
      "Epoch 36/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0578 - custom_layer_accuracy: 0.9823 - custom_layer_1_accuracy: 0.9870Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  59.35288169868554  Class Avg =  59.40664229127705\n",
      "276/276 [==============================] - 102s 370ms/step - loss: 0.0578 - custom_layer_accuracy: 0.9823 - custom_layer_1_accuracy: 0.9870\n",
      "\n",
      "Epoch 00037: LearningRateScheduler reducing learning rate to 0.00010022765486804854.\n",
      "Epoch 37/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0548 - custom_layer_accuracy: 0.9833 - custom_layer_1_accuracy: 0.9870Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  59.45399393326593  Class Avg =  59.51092800554447\n",
      "276/276 [==============================] - 102s 371ms/step - loss: 0.0548 - custom_layer_accuracy: 0.9833 - custom_layer_1_accuracy: 0.9870\n",
      "\n",
      "Epoch 00038: LearningRateScheduler reducing learning rate to 9.915032742026338e-05.\n",
      "Epoch 38/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0549 - custom_layer_accuracy: 0.9865 - custom_layer_1_accuracy: 0.9862Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  59.386585776879  Class Avg =  59.42470950920986\n",
      "276/276 [==============================] - 102s 369ms/step - loss: 0.0549 - custom_layer_accuracy: 0.9865 - custom_layer_1_accuracy: 0.9862\n",
      "\n",
      "Epoch 00039: LearningRateScheduler reducing learning rate to 0.00010166677008099981.\n",
      "Epoch 39/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0604 - custom_layer_accuracy: 0.9828 - custom_layer_1_accuracy: 0.9858Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  59.45399393326593  Class Avg =  59.502328556813715\n",
      "276/276 [==============================] - 102s 369ms/step - loss: 0.0604 - custom_layer_accuracy: 0.9828 - custom_layer_1_accuracy: 0.9858\n",
      "\n",
      "Epoch 00040: LearningRateScheduler reducing learning rate to 9.913377721291555e-05.\n",
      "Epoch 40/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0542 - custom_layer_accuracy: 0.9823 - custom_layer_1_accuracy: 0.9883Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  59.487698011459386  Class Avg =  59.53012516697972\n",
      "276/276 [==============================] - 102s 370ms/step - loss: 0.0542 - custom_layer_accuracy: 0.9823 - custom_layer_1_accuracy: 0.9883\n",
      "\n",
      "Epoch 00041: LearningRateScheduler reducing learning rate to 9.958339443531883e-06.\n",
      "Epoch 41/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0523 - custom_layer_accuracy: 0.9846 - custom_layer_1_accuracy: 0.9884Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  59.487698011459386  Class Avg =  59.52878537682699\n",
      "276/276 [==============================] - 101s 367ms/step - loss: 0.0523 - custom_layer_accuracy: 0.9846 - custom_layer_1_accuracy: 0.9884\n",
      "\n",
      "Epoch 00042: LearningRateScheduler reducing learning rate to 9.899472366138757e-06.\n",
      "Epoch 42/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0550 - custom_layer_accuracy: 0.9854 - custom_layer_1_accuracy: 0.9882Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  59.656218402426695  Class Avg =  59.702683681881325\n",
      "276/276 [==============================] - 101s 367ms/step - loss: 0.0550 - custom_layer_accuracy: 0.9854 - custom_layer_1_accuracy: 0.9882\n",
      "\n",
      "Epoch 00043: LearningRateScheduler reducing learning rate to 9.927848807425154e-06.\n",
      "Epoch 43/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0557 - custom_layer_accuracy: 0.9842 - custom_layer_1_accuracy: 0.9880Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  59.45399393326593  Class Avg =  59.49488707174799\n",
      "276/276 [==============================] - 102s 369ms/step - loss: 0.0557 - custom_layer_accuracy: 0.9842 - custom_layer_1_accuracy: 0.9880\n",
      "\n",
      "Epoch 00044: LearningRateScheduler reducing learning rate to 9.76868686256508e-06.\n",
      "Epoch 44/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0526 - custom_layer_accuracy: 0.9865 - custom_layer_1_accuracy: 0.9892Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  59.487698011459386  Class Avg =  59.52878537682699\n",
      "276/276 [==============================] - 102s 368ms/step - loss: 0.0526 - custom_layer_accuracy: 0.9865 - custom_layer_1_accuracy: 0.9892\n",
      "\n",
      "Epoch 00045: LearningRateScheduler reducing learning rate to 1.0094302690211983e-05.\n",
      "Epoch 45/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0541 - custom_layer_accuracy: 0.9842 - custom_layer_1_accuracy: 0.9883Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  59.55510616784631  Class Avg =  59.599172786015416\n",
      "276/276 [==============================] - 102s 368ms/step - loss: 0.0541 - custom_layer_accuracy: 0.9842 - custom_layer_1_accuracy: 0.9883\n",
      "\n",
      "Epoch 00046: LearningRateScheduler reducing learning rate to 1.0155100616812979e-05.\n",
      "Epoch 46/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0530 - custom_layer_accuracy: 0.9844 - custom_layer_1_accuracy: 0.9889Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  59.55510616784631  Class Avg =  59.594201034604865\n",
      "276/276 [==============================] - 102s 370ms/step - loss: 0.0530 - custom_layer_accuracy: 0.9844 - custom_layer_1_accuracy: 0.9889\n",
      "\n",
      "Epoch 00047: LearningRateScheduler reducing learning rate to 1.0104360221266698e-05.\n",
      "Epoch 47/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0508 - custom_layer_accuracy: 0.9846 - custom_layer_1_accuracy: 0.9893Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  59.45399393326593  Class Avg =  59.49622686190072\n",
      "276/276 [==============================] - 102s 370ms/step - loss: 0.0508 - custom_layer_accuracy: 0.9846 - custom_layer_1_accuracy: 0.9893\n",
      "\n",
      "Epoch 00048: LearningRateScheduler reducing learning rate to 1.0058331913761503e-05.\n",
      "Epoch 48/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0537 - custom_layer_accuracy: 0.9851 - custom_layer_1_accuracy: 0.9887Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  59.487698011459386  Class Avg =  59.52717497891075\n",
      "276/276 [==============================] - 102s 371ms/step - loss: 0.0537 - custom_layer_accuracy: 0.9851 - custom_layer_1_accuracy: 0.9887\n",
      "\n",
      "Epoch 00049: LearningRateScheduler reducing learning rate to 9.887052744810225e-06.\n",
      "Epoch 49/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0533 - custom_layer_accuracy: 0.9861 - custom_layer_1_accuracy: 0.9884Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  59.521402089652845  Class Avg =  59.561553738403546\n",
      "276/276 [==============================] - 103s 372ms/step - loss: 0.0533 - custom_layer_accuracy: 0.9861 - custom_layer_1_accuracy: 0.9884\n",
      "\n",
      "Epoch 00050: LearningRateScheduler reducing learning rate to 1.0133409618939084e-05.\n",
      "Epoch 50/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0536 - custom_layer_accuracy: 0.9857 - custom_layer_1_accuracy: 0.9875Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  59.521402089652845  Class Avg =  59.56256688046838\n",
      "276/276 [==============================] - 103s 372ms/step - loss: 0.0536 - custom_layer_accuracy: 0.9857 - custom_layer_1_accuracy: 0.9875\n",
      "\n",
      "Epoch 00051: LearningRateScheduler reducing learning rate to 9.938906566438784e-06.\n",
      "Epoch 51/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0541 - custom_layer_accuracy: 0.9848 - custom_layer_1_accuracy: 0.9882Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  59.31917762049208  Class Avg =  59.36051257620949\n",
      "276/276 [==============================] - 102s 371ms/step - loss: 0.0541 - custom_layer_accuracy: 0.9848 - custom_layer_1_accuracy: 0.9882\n",
      "\n",
      "Epoch 00052: LearningRateScheduler reducing learning rate to 9.704194039533185e-06.\n",
      "Epoch 52/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0556 - custom_layer_accuracy: 0.9841 - custom_layer_1_accuracy: 0.9867Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  59.521402089652845  Class Avg =  59.55917278602341\n",
      "276/276 [==============================] - 102s 371ms/step - loss: 0.0556 - custom_layer_accuracy: 0.9841 - custom_layer_1_accuracy: 0.9867\n",
      "\n",
      "Epoch 00053: LearningRateScheduler reducing learning rate to 9.854172839073326e-06.\n",
      "Epoch 53/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0548 - custom_layer_accuracy: 0.9836 - custom_layer_1_accuracy: 0.9875Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  59.487698011459386  Class Avg =  59.52018592809625\n",
      "276/276 [==============================] - 102s 371ms/step - loss: 0.0548 - custom_layer_accuracy: 0.9836 - custom_layer_1_accuracy: 0.9875\n",
      "\n",
      "Epoch 00054: LearningRateScheduler reducing learning rate to 1.0147863018396033e-05.\n",
      "Epoch 54/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0562 - custom_layer_accuracy: 0.9832 - custom_layer_1_accuracy: 0.9868Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  59.35288169868554  Class Avg =  59.396226861917405\n",
      "276/276 [==============================] - 102s 371ms/step - loss: 0.0562 - custom_layer_accuracy: 0.9832 - custom_layer_1_accuracy: 0.9868\n",
      "\n",
      "Epoch 00055: LearningRateScheduler reducing learning rate to 9.967902867620033e-06.\n",
      "Epoch 55/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0530 - custom_layer_accuracy: 0.9833 - custom_layer_1_accuracy: 0.9896Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  59.45399393326593  Class Avg =  59.49497585302304\n",
      "276/276 [==============================] - 102s 371ms/step - loss: 0.0530 - custom_layer_accuracy: 0.9833 - custom_layer_1_accuracy: 0.9896\n",
      "\n",
      "Epoch 00056: LearningRateScheduler reducing learning rate to 1.0098913802214873e-05.\n",
      "Epoch 56/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0536 - custom_layer_accuracy: 0.9833 - custom_layer_1_accuracy: 0.9899Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  59.55510616784631  Class Avg =  59.599172786015416\n",
      "276/276 [==============================] - 102s 371ms/step - loss: 0.0536 - custom_layer_accuracy: 0.9833 - custom_layer_1_accuracy: 0.9899\n",
      "\n",
      "Epoch 00057: LearningRateScheduler reducing learning rate to 9.654883400345564e-06.\n",
      "Epoch 57/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0556 - custom_layer_accuracy: 0.9845 - custom_layer_1_accuracy: 0.9865Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  59.45399393326593  Class Avg =  59.49610579652549\n",
      "276/276 [==============================] - 102s 370ms/step - loss: 0.0556 - custom_layer_accuracy: 0.9845 - custom_layer_1_accuracy: 0.9865\n",
      "\n",
      "Epoch 00058: LearningRateScheduler reducing learning rate to 1.0039753439644757e-05.\n",
      "Epoch 58/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0555 - custom_layer_accuracy: 0.9848 - custom_layer_1_accuracy: 0.9876Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  59.35288169868554  Class Avg =  59.393845909537255\n",
      "276/276 [==============================] - 102s 370ms/step - loss: 0.0555 - custom_layer_accuracy: 0.9848 - custom_layer_1_accuracy: 0.9876\n",
      "\n",
      "Epoch 00059: LearningRateScheduler reducing learning rate to 9.770472681705012e-06.\n",
      "Epoch 59/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0534 - custom_layer_accuracy: 0.9854 - custom_layer_1_accuracy: 0.9880Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  59.42028985507246  Class Avg =  59.46527448095308\n",
      "276/276 [==============================] - 103s 372ms/step - loss: 0.0534 - custom_layer_accuracy: 0.9854 - custom_layer_1_accuracy: 0.9880\n",
      "\n",
      "Epoch 00060: LearningRateScheduler reducing learning rate to 1.0177699371417425e-05.\n",
      "Epoch 60/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0543 - custom_layer_accuracy: 0.9851 - custom_layer_1_accuracy: 0.9883Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  59.487698011459386  Class Avg =  59.53432209998877\n",
      "276/276 [==============================] - 102s 371ms/step - loss: 0.0543 - custom_layer_accuracy: 0.9851 - custom_layer_1_accuracy: 0.9883\n",
      "\n",
      "Epoch 00061: LearningRateScheduler reducing learning rate to 1.0057356309352424e-06.\n",
      "Epoch 61/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0506 - custom_layer_accuracy: 0.9851 - custom_layer_1_accuracy: 0.9893Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  59.55510616784631  Class Avg =  59.593636062853626\n",
      "276/276 [==============================] - 102s 370ms/step - loss: 0.0506 - custom_layer_accuracy: 0.9851 - custom_layer_1_accuracy: 0.9893\n",
      "\n",
      "Epoch 00062: LearningRateScheduler reducing learning rate to 1.0004724367305818e-06.\n",
      "Epoch 62/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0534 - custom_layer_accuracy: 0.9845 - custom_layer_1_accuracy: 0.9882Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  59.35288169868554  Class Avg =  59.38944339381556\n",
      "276/276 [==============================] - 102s 371ms/step - loss: 0.0534 - custom_layer_accuracy: 0.9845 - custom_layer_1_accuracy: 0.9882\n",
      "\n",
      "Epoch 00063: LearningRateScheduler reducing learning rate to 1.0143706503337928e-06.\n",
      "Epoch 63/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0533 - custom_layer_accuracy: 0.9846 - custom_layer_1_accuracy: 0.9884Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  59.55510616784631  Class Avg =  59.59441088125515\n",
      "276/276 [==============================] - 103s 371ms/step - loss: 0.0533 - custom_layer_accuracy: 0.9846 - custom_layer_1_accuracy: 0.9884\n",
      "\n",
      "Epoch 00064: LearningRateScheduler reducing learning rate to 1.004150881684304e-06.\n",
      "Epoch 64/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0535 - custom_layer_accuracy: 0.9830 - custom_layer_1_accuracy: 0.9891Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  59.487698011459386  Class Avg =  59.52640442444685\n",
      "276/276 [==============================] - 102s 370ms/step - loss: 0.0535 - custom_layer_accuracy: 0.9830 - custom_layer_1_accuracy: 0.9891\n",
      "\n",
      "Epoch 00065: LearningRateScheduler reducing learning rate to 1.0342223781708341e-06.\n",
      "Epoch 65/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0534 - custom_layer_accuracy: 0.9850 - custom_layer_1_accuracy: 0.9873Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  59.58881024603977  Class Avg =  59.6355408247496\n",
      "276/276 [==============================] - 102s 370ms/step - loss: 0.0534 - custom_layer_accuracy: 0.9850 - custom_layer_1_accuracy: 0.9873\n",
      "\n",
      "Epoch 00066: LearningRateScheduler reducing learning rate to 9.897662459538284e-07.\n",
      "Epoch 66/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0543 - custom_layer_accuracy: 0.9827 - custom_layer_1_accuracy: 0.9871Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  59.25176946410516  Class Avg =  59.29202992892502\n",
      "276/276 [==============================] - 102s 370ms/step - loss: 0.0543 - custom_layer_accuracy: 0.9827 - custom_layer_1_accuracy: 0.9871\n",
      "\n",
      "Epoch 00067: LearningRateScheduler reducing learning rate to 9.839205373482297e-07.\n",
      "Epoch 67/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0512 - custom_layer_accuracy: 0.9841 - custom_layer_1_accuracy: 0.9892Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  59.656218402426695  Class Avg =  59.70324865363254\n",
      "276/276 [==============================] - 102s 370ms/step - loss: 0.0512 - custom_layer_accuracy: 0.9841 - custom_layer_1_accuracy: 0.9892\n",
      "\n",
      "Epoch 00068: LearningRateScheduler reducing learning rate to 1.0017113574460578e-06.\n",
      "Epoch 68/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0561 - custom_layer_accuracy: 0.9828 - custom_layer_1_accuracy: 0.9863Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  59.521402089652845  Class Avg =  59.56232429286743\n",
      "276/276 [==============================] - 103s 372ms/step - loss: 0.0561 - custom_layer_accuracy: 0.9828 - custom_layer_1_accuracy: 0.9863\n",
      "\n",
      "Epoch 00069: LearningRateScheduler reducing learning rate to 1.0171973540335155e-06.\n",
      "Epoch 69/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0532 - custom_layer_accuracy: 0.9834 - custom_layer_1_accuracy: 0.9897Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  59.386585776879  Class Avg =  59.43432210000544\n",
      "276/276 [==============================] - 102s 370ms/step - loss: 0.0532 - custom_layer_accuracy: 0.9834 - custom_layer_1_accuracy: 0.9897\n",
      "\n",
      "Epoch 00070: LearningRateScheduler reducing learning rate to 1.0284197550050737e-06.\n",
      "Epoch 70/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0537 - custom_layer_accuracy: 0.9845 - custom_layer_1_accuracy: 0.9896Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  59.45399393326593  Class Avg =  59.49973775778331\n",
      "276/276 [==============================] - 102s 370ms/step - loss: 0.0537 - custom_layer_accuracy: 0.9845 - custom_layer_1_accuracy: 0.9896\n",
      "\n",
      "Epoch 00071: LearningRateScheduler reducing learning rate to 9.732278916453007e-07.\n",
      "Epoch 71/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0571 - custom_layer_accuracy: 0.9814 - custom_layer_1_accuracy: 0.9873Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  59.55510616784631  Class Avg =  59.600302729517864\n",
      "276/276 [==============================] - 102s 371ms/step - loss: 0.0571 - custom_layer_accuracy: 0.9814 - custom_layer_1_accuracy: 0.9873\n",
      "\n",
      "Epoch 00072: LearningRateScheduler reducing learning rate to 1.0365511661347111e-06.\n",
      "Epoch 72/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0528 - custom_layer_accuracy: 0.9851 - custom_layer_1_accuracy: 0.9887Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  59.184361307718234  Class Avg =  59.2233416989279\n",
      "276/276 [==============================] - 102s 370ms/step - loss: 0.0528 - custom_layer_accuracy: 0.9851 - custom_layer_1_accuracy: 0.9887\n",
      "\n",
      "Epoch 00073: LearningRateScheduler reducing learning rate to 9.765991108830982e-07.\n",
      "Epoch 73/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0523 - custom_layer_accuracy: 0.9842 - custom_layer_1_accuracy: 0.9899Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  59.487698011459386  Class Avg =  59.53012516697973\n",
      "276/276 [==============================] - 102s 370ms/step - loss: 0.0523 - custom_layer_accuracy: 0.9842 - custom_layer_1_accuracy: 0.9899\n",
      "\n",
      "Epoch 00074: LearningRateScheduler reducing learning rate to 1.0268885487748387e-06.\n",
      "Epoch 74/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0549 - custom_layer_accuracy: 0.9842 - custom_layer_1_accuracy: 0.9893Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  59.42028985507246  Class Avg =  59.460512576192826\n",
      "276/276 [==============================] - 102s 370ms/step - loss: 0.0549 - custom_layer_accuracy: 0.9842 - custom_layer_1_accuracy: 0.9893\n",
      "\n",
      "Epoch 00075: LearningRateScheduler reducing learning rate to 1.0195374922317992e-06.\n",
      "Epoch 75/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0542 - custom_layer_accuracy: 0.9840 - custom_layer_1_accuracy: 0.9898Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  59.521402089652845  Class Avg =  59.56051257617615\n",
      "276/276 [==============================] - 102s 370ms/step - loss: 0.0542 - custom_layer_accuracy: 0.9840 - custom_layer_1_accuracy: 0.9898\n",
      "\n",
      "Epoch 00076: LearningRateScheduler reducing learning rate to 1.0303965732986688e-06.\n",
      "Epoch 76/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0539 - custom_layer_accuracy: 0.9841 - custom_layer_1_accuracy: 0.9883Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  59.58881024603977  Class Avg =  59.62640442443019\n",
      "276/276 [==============================] - 102s 371ms/step - loss: 0.0539 - custom_layer_accuracy: 0.9841 - custom_layer_1_accuracy: 0.9883\n",
      "\n",
      "Epoch 00077: LearningRateScheduler reducing learning rate to 9.943886477063367e-07.\n",
      "Epoch 77/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0539 - custom_layer_accuracy: 0.9861 - custom_layer_1_accuracy: 0.9872Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  59.42028985507246  Class Avg =  59.4597377577913\n",
      "276/276 [==============================] - 102s 371ms/step - loss: 0.0539 - custom_layer_accuracy: 0.9861 - custom_layer_1_accuracy: 0.9872\n",
      "\n",
      "Epoch 00078: LearningRateScheduler reducing learning rate to 9.814734177661435e-07.\n",
      "Epoch 78/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0559 - custom_layer_accuracy: 0.9827 - custom_layer_1_accuracy: 0.9872Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  59.58881024603977  Class Avg =  59.6282204050591\n",
      "276/276 [==============================] - 102s 370ms/step - loss: 0.0559 - custom_layer_accuracy: 0.9827 - custom_layer_1_accuracy: 0.9872\n",
      "\n",
      "Epoch 00079: LearningRateScheduler reducing learning rate to 1.0054643369477655e-06.\n",
      "Epoch 79/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0503 - custom_layer_accuracy: 0.9862 - custom_layer_1_accuracy: 0.9904Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  59.31917762049208  Class Avg =  59.36051257620949\n",
      "276/276 [==============================] - 102s 370ms/step - loss: 0.0503 - custom_layer_accuracy: 0.9862 - custom_layer_1_accuracy: 0.9904\n",
      "\n",
      "Epoch 00080: LearningRateScheduler reducing learning rate to 1.009731178720339e-06.\n",
      "Epoch 80/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0531 - custom_layer_accuracy: 0.9861 - custom_layer_1_accuracy: 0.9871Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  59.521402089652845  Class Avg =  59.568785376818994\n",
      "276/276 [==============================] - 102s 371ms/step - loss: 0.0531 - custom_layer_accuracy: 0.9861 - custom_layer_1_accuracy: 0.9871\n",
      "\n",
      "Epoch 00081: LearningRateScheduler reducing learning rate to 9.636665086275029e-08.\n",
      "Epoch 81/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0535 - custom_layer_accuracy: 0.9844 - custom_layer_1_accuracy: 0.9870Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  59.42028985507246  Class Avg =  59.459947604441595\n",
      "276/276 [==============================] - 102s 370ms/step - loss: 0.0535 - custom_layer_accuracy: 0.9844 - custom_layer_1_accuracy: 0.9870\n",
      "\n",
      "Epoch 00082: LearningRateScheduler reducing learning rate to 9.965403989288446e-08.\n",
      "Epoch 82/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0532 - custom_layer_accuracy: 0.9848 - custom_layer_1_accuracy: 0.9868Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  59.487698011459386  Class Avg =  59.53069013873094\n",
      "276/276 [==============================] - 102s 369ms/step - loss: 0.0532 - custom_layer_accuracy: 0.9848 - custom_layer_1_accuracy: 0.9868\n",
      "\n",
      "Epoch 00083: LearningRateScheduler reducing learning rate to 9.904801897682711e-08.\n",
      "Epoch 83/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0528 - custom_layer_accuracy: 0.9845 - custom_layer_1_accuracy: 0.9893Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  59.35288169868554  Class Avg =  59.389322328440315\n",
      "276/276 [==============================] - 102s 369ms/step - loss: 0.0528 - custom_layer_accuracy: 0.9845 - custom_layer_1_accuracy: 0.9893\n",
      "\n",
      "Epoch 00084: LearningRateScheduler reducing learning rate to 1.0178233762765575e-07.\n",
      "Epoch 84/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0533 - custom_layer_accuracy: 0.9834 - custom_layer_1_accuracy: 0.9886Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  59.45399393326593  Class Avg =  59.49113830905256\n",
      "276/276 [==============================] - 102s 368ms/step - loss: 0.0533 - custom_layer_accuracy: 0.9834 - custom_layer_1_accuracy: 0.9886\n",
      "\n",
      "Epoch 00085: LearningRateScheduler reducing learning rate to 1.0192662399457144e-07.\n",
      "Epoch 85/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0550 - custom_layer_accuracy: 0.9833 - custom_layer_1_accuracy: 0.9879Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  59.45399393326593  Class Avg =  59.493757128245555\n",
      "276/276 [==============================] - 102s 368ms/step - loss: 0.0550 - custom_layer_accuracy: 0.9833 - custom_layer_1_accuracy: 0.9879\n",
      "\n",
      "Epoch 00086: LearningRateScheduler reducing learning rate to 9.796141390752579e-08.\n",
      "Epoch 86/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0530 - custom_layer_accuracy: 0.9831 - custom_layer_1_accuracy: 0.9880Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  59.35288169868554  Class Avg =  59.393845909537255\n",
      "276/276 [==============================] - 102s 368ms/step - loss: 0.0530 - custom_layer_accuracy: 0.9831 - custom_layer_1_accuracy: 0.9880\n",
      "\n",
      "Epoch 00087: LearningRateScheduler reducing learning rate to 1.0287329019958452e-07.\n",
      "Epoch 87/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0518 - custom_layer_accuracy: 0.9849 - custom_layer_1_accuracy: 0.9883Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  59.487698011459386  Class Avg =  59.525839452695635\n",
      "276/276 [==============================] - 102s 368ms/step - loss: 0.0518 - custom_layer_accuracy: 0.9849 - custom_layer_1_accuracy: 0.9883\n",
      "\n",
      "Epoch 00088: LearningRateScheduler reducing learning rate to 1.0117424951802684e-07.\n",
      "Epoch 88/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0518 - custom_layer_accuracy: 0.9832 - custom_layer_1_accuracy: 0.9887Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  59.42028985507246  Class Avg =  59.459826539066356\n",
      "276/276 [==============================] - 102s 368ms/step - loss: 0.0518 - custom_layer_accuracy: 0.9832 - custom_layer_1_accuracy: 0.9887\n",
      "\n",
      "Epoch 00089: LearningRateScheduler reducing learning rate to 1.0195999360570423e-07.\n",
      "Epoch 89/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0557 - custom_layer_accuracy: 0.9831 - custom_layer_1_accuracy: 0.9865Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  59.62251432423323  Class Avg =  59.66822040505111\n",
      "276/276 [==============================] - 102s 369ms/step - loss: 0.0557 - custom_layer_accuracy: 0.9831 - custom_layer_1_accuracy: 0.9865\n",
      "\n",
      "Epoch 00090: LearningRateScheduler reducing learning rate to 9.824948019770652e-08.\n",
      "Epoch 90/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0564 - custom_layer_accuracy: 0.9832 - custom_layer_1_accuracy: 0.9872Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  59.487698011459386  Class Avg =  59.530690138730954\n",
      "276/276 [==============================] - 102s 368ms/step - loss: 0.0564 - custom_layer_accuracy: 0.9832 - custom_layer_1_accuracy: 0.9872\n",
      "\n",
      "Epoch 00091: LearningRateScheduler reducing learning rate to 1.0008731079097796e-07.\n",
      "Epoch 91/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0506 - custom_layer_accuracy: 0.9850 - custom_layer_1_accuracy: 0.9895Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  59.386585776879  Class Avg =  59.42717924286504\n",
      "276/276 [==============================] - 102s 368ms/step - loss: 0.0506 - custom_layer_accuracy: 0.9850 - custom_layer_1_accuracy: 0.9895\n",
      "\n",
      "Epoch 00092: LearningRateScheduler reducing learning rate to 9.747995253046237e-08.\n",
      "Epoch 92/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0561 - custom_layer_accuracy: 0.9845 - custom_layer_1_accuracy: 0.9858Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  59.487698011459386  Class Avg =  59.53069013873094\n",
      "276/276 [==============================] - 101s 367ms/step - loss: 0.0561 - custom_layer_accuracy: 0.9845 - custom_layer_1_accuracy: 0.9858\n",
      "\n",
      "Epoch 00093: LearningRateScheduler reducing learning rate to 1.0108586910179288e-07.\n",
      "Epoch 93/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0564 - custom_layer_accuracy: 0.9831 - custom_layer_1_accuracy: 0.9881Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  59.25176946410516  Class Avg =  59.28560158592411\n",
      "276/276 [==============================] - 101s 367ms/step - loss: 0.0564 - custom_layer_accuracy: 0.9831 - custom_layer_1_accuracy: 0.9881\n",
      "\n",
      "Epoch 00094: LearningRateScheduler reducing learning rate to 1.0034487330681395e-07.\n",
      "Epoch 94/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0549 - custom_layer_accuracy: 0.9829 - custom_layer_1_accuracy: 0.9874Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  59.62251432423323  Class Avg =  59.66086770126042\n",
      "276/276 [==============================] - 101s 367ms/step - loss: 0.0549 - custom_layer_accuracy: 0.9829 - custom_layer_1_accuracy: 0.9874\n",
      "\n",
      "Epoch 00095: LearningRateScheduler reducing learning rate to 9.743194316442212e-08.\n",
      "Epoch 95/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0562 - custom_layer_accuracy: 0.9832 - custom_layer_1_accuracy: 0.9875Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  59.521402089652845  Class Avg =  59.56402347205872\n",
      "276/276 [==============================] - 101s 367ms/step - loss: 0.0562 - custom_layer_accuracy: 0.9832 - custom_layer_1_accuracy: 0.9875\n",
      "\n",
      "Epoch 00096: LearningRateScheduler reducing learning rate to 1.0154400132052595e-07.\n",
      "Epoch 96/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0525 - custom_layer_accuracy: 0.9846 - custom_layer_1_accuracy: 0.9899Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  59.42028985507246  Class Avg =  59.45917278604009\n",
      "276/276 [==============================] - 101s 367ms/step - loss: 0.0525 - custom_layer_accuracy: 0.9846 - custom_layer_1_accuracy: 0.9899\n",
      "\n",
      "Epoch 00097: LearningRateScheduler reducing learning rate to 9.812609423523334e-08.\n",
      "Epoch 97/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0528 - custom_layer_accuracy: 0.9851 - custom_layer_1_accuracy: 0.9874Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  59.487698011459386  Class Avg =  59.52515341556918\n",
      "276/276 [==============================] - 101s 367ms/step - loss: 0.0528 - custom_layer_accuracy: 0.9851 - custom_layer_1_accuracy: 0.9874\n",
      "\n",
      "Epoch 00098: LearningRateScheduler reducing learning rate to 9.734067213742274e-08.\n",
      "Epoch 98/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0566 - custom_layer_accuracy: 0.9836 - custom_layer_1_accuracy: 0.9878Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  59.487698011459386  Class Avg =  59.536908635081566\n",
      "276/276 [==============================] - 101s 367ms/step - loss: 0.0566 - custom_layer_accuracy: 0.9836 - custom_layer_1_accuracy: 0.9878\n",
      "\n",
      "Epoch 00099: LearningRateScheduler reducing learning rate to 1.006906020502066e-07.\n",
      "Epoch 99/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0533 - custom_layer_accuracy: 0.9849 - custom_layer_1_accuracy: 0.9896Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  59.656218402426695  Class Avg =  59.7003027295012\n",
      "276/276 [==============================] - 102s 368ms/step - loss: 0.0533 - custom_layer_accuracy: 0.9849 - custom_layer_1_accuracy: 0.9896\n",
      "\n",
      "Epoch 00100: LearningRateScheduler reducing learning rate to 9.976725238282776e-08.\n",
      "Epoch 100/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0543 - custom_layer_accuracy: 0.9814 - custom_layer_1_accuracy: 0.9888Predictions Using Average result\n",
      "Epocs =  2966  Top1 =  59.521402089652845  Class Avg =  59.56583945268764\n",
      "276/276 [==============================] - 102s 369ms/step - loss: 0.0543 - custom_layer_accuracy: 0.9814 - custom_layer_1_accuracy: 0.9888\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "892"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top1 = AverageMeter()\n",
    "class_avg = ClassAverageMeter(50)\n",
    "class CustomCallback(keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        top1.reset()\n",
    "        class_avg.reset(50)\n",
    "        predictions = model.predict((test_data, final_test_labels_cat, test_text_features, testing_phase))\n",
    "        print(\"Predictions Using Average result\")\n",
    "        avg_output = (predictions[0]+predictions[1])/2\n",
    "        prec1,class_acc,class_cnt,prec_prob = accuracy(avg_output, final_test_labels, 50)\n",
    "\n",
    "        top1.update(prec1, avg_output.shape[0])\n",
    "        class_avg.update(class_acc,class_cnt,prec_prob)\n",
    "\n",
    "        print(\"Epocs = \", i, \" Top1 = \", top1.avg, \" Class Avg = \", class_avg.avg)\n",
    "        del predictions\n",
    "        gc.collect()\n",
    "\n",
    "drop = 20\n",
    "def sgdr(epoch):\n",
    "    lr = learning_rate*(0.5 ** (epoch // drop))\n",
    "    return lr\n",
    "\n",
    "reduce_lr = tf.keras.callbacks.LearningRateScheduler(sgdr, verbose=True)\n",
    "on_epoch_callback = CustomCallback()\n",
    "\n",
    "training_phase = np.ones(shape=len(train_data), dtype=np.int64)\n",
    "final_train_labels_shuffled_cat = tf.keras.utils.to_categorical(final_train_labels_shuffled, num_classes=150)\n",
    "final_test_labels_cat = tf.keras.utils.to_categorical(final_test_labels, num_classes=50)\n",
    "\n",
    "model.fit((train_data_shuffled, final_train_labels_shuffled_cat, train_text_features_shuffled, training_phase),\n",
    "            final_train_labels_shuffled_cat, epochs=100, batch_size=32,\n",
    "            callbacks=[reduce_lr, on_epoch_callback])\n",
    "\n",
    "\n",
    "\n",
    "gc.collect()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
